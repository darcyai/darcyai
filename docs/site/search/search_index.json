{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darcy AI Engine Darcy AI Engine is a Python library that makes building AI apps as easy as building any other type of app. AI Engine exposes high-level constructs ( InputStream , Perceptor , Callback , OutputStream ) that you assemble in a Pipeline with a few lines of Python. To get started, see the Build Guide , look at the examples , and consult the Python reference docs . Example This code (from basic.py ) shows how easy it is to create a minimal AI Engine pipeline: #Add the DarcyAI components that we need, particularly the InputStream, OutputStream, Pipeline, and PerceptorMock from darcyai.tests.perceptor_mock import PerceptorMock from darcyai.pipeline import Pipeline from sample_input_stream import SampleInputStream from sample_output_stream import SampleOutputStream #Define a class to hold all of our operations class SingleStreamDemo(): def __init__(self): #Create an input stream and an output stream that we can use in our demo ping = SampleInputStream() output_stream = SampleOutputStream() #Give our class a pipeline property and instantiate it with a Darcy AI pipeline self.__pipeline = Pipeline(input_stream=ping, input_stream_error_handler_callback=self.__input_stream_error_handler_callback, universal_rest_api=True, rest_api_base_path=\"/pipeline\", rest_api_host=\"0.0.0.0\", rest_api_port=8080) #Add our output stream to the pipeline self.__pipeline.add_output_stream(\"output\", self.__output_stream_callback, output_stream) #Create a mock perceptor and add it to the pipeline p1 = PerceptorMock() self.__pipeline.add_perceptor(\"p1\", p1, accelerator_idx=0, input_callback=self.__perceptor_input_callback) #Define a \"run\" method that just calls \"run\" on the pipeline def run(self): self.__pipeline.run() #Define an input callback for the mock perceptor that just sends the data onward with no manipulation def __perceptor_input_callback(self, input_data, pom, config): return input_data #Define an output stream callback that does not manipulate the data def __output_stream_callback(self, pom, input_data): pass #Define an input stream error handler callback that just continues onward def __input_stream_error_handler_callback(self, exception): pass #In the main thread, start the application by instantiating our demo class and calling \"run\" if __name__ == \"__main__\": single_stream = SingleStreamDemo() single_stream.run() Issues, Contributing, Discussion If you discover issues with AI Engine, view the issues , or create a new one. You can also submit a Pull Request , or join the discussions .","title":"Home"},{"location":"#darcy-ai-engine","text":"Darcy AI Engine is a Python library that makes building AI apps as easy as building any other type of app. AI Engine exposes high-level constructs ( InputStream , Perceptor , Callback , OutputStream ) that you assemble in a Pipeline with a few lines of Python. To get started, see the Build Guide , look at the examples , and consult the Python reference docs .","title":"Darcy AI Engine"},{"location":"#example","text":"This code (from basic.py ) shows how easy it is to create a minimal AI Engine pipeline: #Add the DarcyAI components that we need, particularly the InputStream, OutputStream, Pipeline, and PerceptorMock from darcyai.tests.perceptor_mock import PerceptorMock from darcyai.pipeline import Pipeline from sample_input_stream import SampleInputStream from sample_output_stream import SampleOutputStream #Define a class to hold all of our operations class SingleStreamDemo(): def __init__(self): #Create an input stream and an output stream that we can use in our demo ping = SampleInputStream() output_stream = SampleOutputStream() #Give our class a pipeline property and instantiate it with a Darcy AI pipeline self.__pipeline = Pipeline(input_stream=ping, input_stream_error_handler_callback=self.__input_stream_error_handler_callback, universal_rest_api=True, rest_api_base_path=\"/pipeline\", rest_api_host=\"0.0.0.0\", rest_api_port=8080) #Add our output stream to the pipeline self.__pipeline.add_output_stream(\"output\", self.__output_stream_callback, output_stream) #Create a mock perceptor and add it to the pipeline p1 = PerceptorMock() self.__pipeline.add_perceptor(\"p1\", p1, accelerator_idx=0, input_callback=self.__perceptor_input_callback) #Define a \"run\" method that just calls \"run\" on the pipeline def run(self): self.__pipeline.run() #Define an input callback for the mock perceptor that just sends the data onward with no manipulation def __perceptor_input_callback(self, input_data, pom, config): return input_data #Define an output stream callback that does not manipulate the data def __output_stream_callback(self, pom, input_data): pass #Define an input stream error handler callback that just continues onward def __input_stream_error_handler_callback(self, exception): pass #In the main thread, start the application by instantiating our demo class and calling \"run\" if __name__ == \"__main__\": single_stream = SingleStreamDemo() single_stream.run()","title":"Example"},{"location":"#issues-contributing-discussion","text":"If you discover issues with AI Engine, view the issues , or create a new one. You can also submit a Pull Request , or join the discussions .","title":"Issues, Contributing, Discussion"},{"location":"pipeline/","text":"darcyai.pipeline Pipeline Objects class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\") num_of_edge_tpus def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus() add_perceptor def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: int = 0, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_before def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_after def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_parallel_perceptor def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) update_input_stream def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) remove_output_stream def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\") stop def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop() run def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run() get_pom def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom() get_current_pulse_number def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number() get_latest_input def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input() get_historical_input def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1) get_input_history def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history() get_historical_pom def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1) get_pom_history def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history() run_perceptor def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True) get_graph def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph() get_all_performance_metrics def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics() get_pulse_performance_metrics def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1) get_perceptor_performance_metrics def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1) set_perceptor_config def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1) get_perceptor_config def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\") set_output_stream_config def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1) get_output_stream_config def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"Pipeline"},{"location":"pipeline/#darcyaipipeline","text":"","title":"darcyai.pipeline"},{"location":"pipeline/#pipeline-objects","text":"class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\")","title":"Pipeline Objects"},{"location":"pipeline/#num_of_edge_tpus","text":"def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus()","title":"num_of_edge_tpus"},{"location":"pipeline/#add_perceptor","text":"def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: int = 0, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor"},{"location":"pipeline/#add_perceptor_before","text":"def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_before"},{"location":"pipeline/#add_perceptor_after","text":"def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_after"},{"location":"pipeline/#add_parallel_perceptor","text":"def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_parallel_perceptor"},{"location":"pipeline/#update_input_stream","text":"def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"})","title":"update_input_stream"},{"location":"pipeline/#remove_output_stream","text":"def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\")","title":"remove_output_stream"},{"location":"pipeline/#stop","text":"def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop()","title":"stop"},{"location":"pipeline/#run","text":"def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run()","title":"run"},{"location":"pipeline/#get_pom","text":"def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom()","title":"get_pom"},{"location":"pipeline/#get_current_pulse_number","text":"def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number()","title":"get_current_pulse_number"},{"location":"pipeline/#get_latest_input","text":"def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input()","title":"get_latest_input"},{"location":"pipeline/#get_historical_input","text":"def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1)","title":"get_historical_input"},{"location":"pipeline/#get_input_history","text":"def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history()","title":"get_input_history"},{"location":"pipeline/#get_historical_pom","text":"def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1)","title":"get_historical_pom"},{"location":"pipeline/#get_pom_history","text":"def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history()","title":"get_pom_history"},{"location":"pipeline/#run_perceptor","text":"def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True)","title":"run_perceptor"},{"location":"pipeline/#get_graph","text":"def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph()","title":"get_graph"},{"location":"pipeline/#get_all_performance_metrics","text":"def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics()","title":"get_all_performance_metrics"},{"location":"pipeline/#get_pulse_performance_metrics","text":"def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1)","title":"get_pulse_performance_metrics"},{"location":"pipeline/#get_perceptor_performance_metrics","text":"def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1)","title":"get_perceptor_performance_metrics"},{"location":"pipeline/#set_perceptor_config","text":"def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1)","title":"set_perceptor_config"},{"location":"pipeline/#get_perceptor_config","text":"def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\")","title":"get_perceptor_config"},{"location":"pipeline/#set_output_stream_config","text":"def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1)","title":"set_output_stream_config"},{"location":"pipeline/#get_output_stream_config","text":"def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"get_output_stream_config"},{"location":"serializable/","text":"darcyai.serializable Serializable Objects class Serializable() Base class for all serializable objects. serialize def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"Serializable"},{"location":"serializable/#darcyaiserializable","text":"","title":"darcyai.serializable"},{"location":"serializable/#serializable-objects","text":"class Serializable() Base class for all serializable objects.","title":"Serializable Objects"},{"location":"serializable/#serialize","text":"def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"serialize"},{"location":"streamdata/","text":"darcyai.stream_data StreamData Objects class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data. serialize def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"StreamData"},{"location":"streamdata/#darcyaistream_data","text":"","title":"darcyai.stream_data"},{"location":"streamdata/#streamdata-objects","text":"class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data.","title":"StreamData Objects"},{"location":"streamdata/#serialize","text":"def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"serialize"},{"location":"input-streams/camerastream/","text":"darcyai.input.camera_stream CameraStream Objects class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\") stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream() get_video_inputs @staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"CameraStream"},{"location":"input-streams/camerastream/#darcyaiinputcamera_stream","text":"","title":"darcyai.input.camera_stream"},{"location":"input-streams/camerastream/#camerastream-objects","text":"class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\")","title":"CameraStream Objects"},{"location":"input-streams/camerastream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop()","title":"stop"},{"location":"input-streams/camerastream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream()","title":"stream"},{"location":"input-streams/camerastream/#get_video_inputs","text":"@staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"get_video_inputs"},{"location":"input-streams/inputmultistream/","text":"darcyai.input.input_multi_stream InputMultiStream Objects class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback) remove_stream def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\") get_stream def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\") add_stream def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) stream def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream() stop def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"InputMultiStream"},{"location":"input-streams/inputmultistream/#darcyaiinputinput_multi_stream","text":"","title":"darcyai.input.input_multi_stream"},{"location":"input-streams/inputmultistream/#inputmultistream-objects","text":"class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback)","title":"InputMultiStream Objects"},{"location":"input-streams/inputmultistream/#remove_stream","text":"def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\")","title":"remove_stream"},{"location":"input-streams/inputmultistream/#get_stream","text":"def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\")","title":"get_stream"},{"location":"input-streams/inputmultistream/#add_stream","text":"def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera)","title":"add_stream"},{"location":"input-streams/inputmultistream/#stream","text":"def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream()","title":"stream"},{"location":"input-streams/inputmultistream/#stop","text":"def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"stop"},{"location":"input-streams/inputstream/","text":"darcyai.input.input_stream InputStream Objects class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass stream def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input. stop def stop() -> None Stops the stream.","title":"InputStream"},{"location":"input-streams/inputstream/#darcyaiinputinput_stream","text":"","title":"darcyai.input.input_stream"},{"location":"input-streams/inputstream/#inputstream-objects","text":"class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass","title":"InputStream Objects"},{"location":"input-streams/inputstream/#stream","text":"def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input.","title":"stream"},{"location":"input-streams/inputstream/#stop","text":"def stop() -> None Stops the stream.","title":"stop"},{"location":"input-streams/videofilestream/","text":"darcyai.input.video_file_stream VideoFileStream Objects class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"VideoFileStream"},{"location":"input-streams/videofilestream/#darcyaiinputvideo_file_stream","text":"","title":"darcyai.input.video_file_stream"},{"location":"input-streams/videofilestream/#videofilestream-objects","text":"class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True)","title":"VideoFileStream Objects"},{"location":"input-streams/videofilestream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop()","title":"stop"},{"location":"input-streams/videofilestream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"stream"},{"location":"input-streams/videostreamdata/","text":"darcyai.input.video_stream_data VideoStreamData Objects class VideoStreamData(StreamData) StreamData representation of video frames serialize def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"VideoStreamData"},{"location":"input-streams/videostreamdata/#darcyaiinputvideo_stream_data","text":"","title":"darcyai.input.video_stream_data"},{"location":"input-streams/videostreamdata/#videostreamdata-objects","text":"class VideoStreamData(StreamData) StreamData representation of video frames","title":"VideoStreamData Objects"},{"location":"input-streams/videostreamdata/#serialize","text":"def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"serialize"},{"location":"output-streams/csvoutputstream/","text":"darcyai.output.csv_output_stream CSVOutputStream Objects class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) write def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]]) close def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"CSVOutputStream"},{"location":"output-streams/csvoutputstream/#darcyaioutputcsv_output_stream","text":"","title":"darcyai.output.csv_output_stream"},{"location":"output-streams/csvoutputstream/#csvoutputstream-objects","text":"class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0)","title":"CSVOutputStream Objects"},{"location":"output-streams/csvoutputstream/#write","text":"def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]])","title":"write"},{"location":"output-streams/csvoutputstream/#close","text":"def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"close"},{"location":"output-streams/jsonoutputstream/","text":"darcyai.output.json_output_stream JSONOutputStream Objects class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) write def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"}) close def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"JSONOutputStream"},{"location":"output-streams/jsonoutputstream/#darcyaioutputjson_output_stream","text":"","title":"darcyai.output.json_output_stream"},{"location":"output-streams/jsonoutputstream/#jsonoutputstream-objects","text":"class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0)","title":"JSONOutputStream Objects"},{"location":"output-streams/jsonoutputstream/#write","text":"def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"})","title":"write"},{"location":"output-streams/jsonoutputstream/#close","text":"def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/","text":"darcyai.output.live_feed_stream LiveFeedStream Objects class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) write def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame) get_fps def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps() set_fps def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30) get_quality def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality() set_quality def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50) close def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close() get_latest_frame def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"LiveFeedStream"},{"location":"output-streams/livefeedstream/#darcyaioutputlive_feed_stream","text":"","title":"darcyai.output.live_feed_stream"},{"location":"output-streams/livefeedstream/#livefeedstream-objects","text":"class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100)","title":"LiveFeedStream Objects"},{"location":"output-streams/livefeedstream/#write","text":"def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame)","title":"write"},{"location":"output-streams/livefeedstream/#get_fps","text":"def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps()","title":"get_fps"},{"location":"output-streams/livefeedstream/#set_fps","text":"def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30)","title":"set_fps"},{"location":"output-streams/livefeedstream/#get_quality","text":"def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality()","title":"get_quality"},{"location":"output-streams/livefeedstream/#set_quality","text":"def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50)","title":"set_quality"},{"location":"output-streams/livefeedstream/#close","text":"def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/#get_latest_frame","text":"def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"get_latest_frame"},{"location":"output-streams/outputstream/","text":"darcyai.output.output_stream OutputStream Objects class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass write def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream. close def close() -> None Closes the output stream. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"OutputStream"},{"location":"output-streams/outputstream/#darcyaioutputoutput_stream","text":"","title":"darcyai.output.output_stream"},{"location":"output-streams/outputstream/#outputstream-objects","text":"class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass","title":"OutputStream Objects"},{"location":"output-streams/outputstream/#write","text":"def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream.","title":"write"},{"location":"output-streams/outputstream/#close","text":"def close() -> None Closes the output stream.","title":"close"},{"location":"output-streams/outputstream/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"output-streams/outputstream/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"output-streams/outputstream/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"output-streams/restapistream/","text":"darcyai.output.rest_api_stream RestApiStream Objects class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) write def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"}) close def close() -> None Closes the output stream.","title":"RestApiStream"},{"location":"output-streams/restapistream/#darcyaioutputrest_api_stream","text":"","title":"darcyai.output.rest_api_stream"},{"location":"output-streams/restapistream/#restapistream-objects","text":"class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"})","title":"RestApiStream Objects"},{"location":"output-streams/restapistream/#write","text":"def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"})","title":"write"},{"location":"output-streams/restapistream/#close","text":"def close() -> None Closes the output stream.","title":"close"},{"location":"perceptors/class/","text":"darcyai.perceptor.detected_class Class Objects class Class() Class to store detected class information. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. __str__ def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"Class"},{"location":"perceptors/class/#darcyaiperceptordetected_class","text":"","title":"darcyai.perceptor.detected_class"},{"location":"perceptors/class/#class-objects","text":"class Class() Class to store detected class information. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class.","title":"Class Objects"},{"location":"perceptors/class/#__str__","text":"def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"__str__"},{"location":"perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(MultiPlatformPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. __init__ def __init__(processor_preference: dict, threshold: float, top_k: int = None, mean: float = 128.0, std: float = 128.0, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. mean ( float ): The mean of the image (Coral Edge TPU). std ( float ): The standard deviation of the image (Coral Edge TPU). quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.image_classification_perceptor import ImageClassificationPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", // The path to the labels file. }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { // A dictionary of labels. \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } image_classification_perceptor = ImageClassificationPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"ImageClassificationPerceptor"},{"location":"perceptors/imageclassificationperceptor/#darcyaiperceptorimage_classification_perceptor","text":"","title":"darcyai.perceptor.image_classification_perceptor"},{"location":"perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(MultiPlatformPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/imageclassificationperceptor/#__init__","text":"def __init__(processor_preference: dict, threshold: float, top_k: int = None, mean: float = 128.0, std: float = 128.0, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. mean ( float ): The mean of the image (Coral Edge TPU). std ( float ): The standard deviation of the image (Coral Edge TPU). quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.image_classification_perceptor import ImageClassificationPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", // The path to the labels file. }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { // A dictionary of labels. \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } image_classification_perceptor = ImageClassificationPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"__init__"},{"location":"perceptors/object/","text":"darcyai.perceptor.detected_object Object Objects class Object() Class for storing detected objects. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. xmin ( int ): The x-coordinate of the top left corner of the bounding box. ymin ( int ): The y-coordinate of the top left corner of the bounding box. xmax ( int ): The x-coordinate of the bottom right corner of the bounding box. ymax ( int ): The y-coordinate of the bottom right corner of the bounding box. __str__ def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"Object"},{"location":"perceptors/object/#darcyaiperceptordetected_object","text":"","title":"darcyai.perceptor.detected_object"},{"location":"perceptors/object/#object-objects","text":"class Object() Class for storing detected objects. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. xmin ( int ): The x-coordinate of the top left corner of the bounding box. ymin ( int ): The y-coordinate of the top left corner of the bounding box. xmax ( int ): The x-coordinate of the bottom right corner of the bounding box. ymax ( int ): The y-coordinate of the bottom right corner of the bounding box.","title":"Object Objects"},{"location":"perceptors/object/#__str__","text":"def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"__str__"},{"location":"perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(MultiPlatformPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. __init__ def __init__(processor_preference: dict, threshold: float, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.object_detection_perceptor import ObjectDetectionPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } object_detection_perceptor = ObjectDetectionPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"ObjectDetectionPerceptor"},{"location":"perceptors/objectdetectionperceptor/#darcyaiperceptorobject_detection_perceptor","text":"","title":"darcyai.perceptor.object_detection_perceptor"},{"location":"perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(MultiPlatformPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/objectdetectionperceptor/#__init__","text":"def __init__(processor_preference: dict, threshold: float, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.object_detection_perceptor import ObjectDetectionPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } object_detection_perceptor = ObjectDetectionPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"__init__"},{"location":"perceptors/peopleperceptor/","text":"darcyai.perceptor.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(MultiPlatformPerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/peopleperceptor/#darcyaiperceptorpeople_perceptor","text":"","title":"darcyai.perceptor.people_perceptor"},{"location":"perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(MultiPlatformPerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"perceptors/peoplepom/","text":"darcyai.perceptor.people_perceptor_pom PeoplePOM Objects class PeoplePOM(Serializable) People perceptor object model set_annotated_frame def set_annotated_frame(frame) Stores the frame annotated with the persons data Arguments frame ( np.array ): The annotated frame set_raw_frame def set_raw_frame(frame) Stores the frame with the people data Arguments frame ( np.array ): The raw frame set_people def set_people(people) Stores the detected people Arguments people ( list ): The detected people annotatedFrame def annotatedFrame() Returns the annotated frame Returns np.array : The annotated frame rawFrame def rawFrame() Returns the raw frame Returns np.array : The raw frame peopleCount def peopleCount() Returns the number of people detected Returns int : The number of people detected personInFront def personInFront() Returns the person in front of the camera Returns The person in front of the camera people def people() Returns the detected people Returns list : The detected people person def person(person_id) Returns the person with the given id Arguments person_id ( str ): The person id Returns dict : The person faceImage def faceImage(person_id) Returns the face image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The face image bodyImage def bodyImage(person_id) Returns the body image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The body image personImage def personImage(person_id) Returns the image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The person image faceSize def faceSize(person_id) Returns the size of the face of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the face bodySize def bodySize(person_id) Returns the size of the body of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the body wholeSize def wholeSize(person_id) Returns the size of the whole person with the given id Arguments person_id ( str ): The person id Returns int : The size of the whole person bodyPose def bodyPose(person_id) Returns the PoseNet data of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data bodyPoseHistory def bodyPoseHistory(person_id) Returns the PoseNet data history of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data travelPath def travelPath(person_id) Returns the travel path of the person with the given id Arguments person_id ( str ): The person id Returns The travel path timeInView def timeInView(person_id) Returns the time the person with the given id has been in view Arguments person_id ( str ): The person id Returns int : The time in view recentlyDepartedPeople def recentlyDepartedPeople() Returns the people that have recently departed Returns list : The people that have recently departed occludedPeople def occludedPeople() Returns the people that are occluded Returns list : The people that are occluded serialize def serialize() Returns the serialized data of the object Returns dict : The serialized data","title":"PeoplePOM"},{"location":"perceptors/peoplepom/#darcyaiperceptorpeople_perceptor_pom","text":"","title":"darcyai.perceptor.people_perceptor_pom"},{"location":"perceptors/peoplepom/#peoplepom-objects","text":"class PeoplePOM(Serializable) People perceptor object model","title":"PeoplePOM Objects"},{"location":"perceptors/peoplepom/#set_annotated_frame","text":"def set_annotated_frame(frame) Stores the frame annotated with the persons data Arguments frame ( np.array ): The annotated frame","title":"set_annotated_frame"},{"location":"perceptors/peoplepom/#set_raw_frame","text":"def set_raw_frame(frame) Stores the frame with the people data Arguments frame ( np.array ): The raw frame","title":"set_raw_frame"},{"location":"perceptors/peoplepom/#set_people","text":"def set_people(people) Stores the detected people Arguments people ( list ): The detected people","title":"set_people"},{"location":"perceptors/peoplepom/#annotatedframe","text":"def annotatedFrame() Returns the annotated frame Returns np.array : The annotated frame","title":"annotatedFrame"},{"location":"perceptors/peoplepom/#rawframe","text":"def rawFrame() Returns the raw frame Returns np.array : The raw frame","title":"rawFrame"},{"location":"perceptors/peoplepom/#peoplecount","text":"def peopleCount() Returns the number of people detected Returns int : The number of people detected","title":"peopleCount"},{"location":"perceptors/peoplepom/#personinfront","text":"def personInFront() Returns the person in front of the camera Returns The person in front of the camera","title":"personInFront"},{"location":"perceptors/peoplepom/#people","text":"def people() Returns the detected people Returns list : The detected people","title":"people"},{"location":"perceptors/peoplepom/#person","text":"def person(person_id) Returns the person with the given id Arguments person_id ( str ): The person id Returns dict : The person","title":"person"},{"location":"perceptors/peoplepom/#faceimage","text":"def faceImage(person_id) Returns the face image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The face image","title":"faceImage"},{"location":"perceptors/peoplepom/#bodyimage","text":"def bodyImage(person_id) Returns the body image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The body image","title":"bodyImage"},{"location":"perceptors/peoplepom/#personimage","text":"def personImage(person_id) Returns the image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The person image","title":"personImage"},{"location":"perceptors/peoplepom/#facesize","text":"def faceSize(person_id) Returns the size of the face of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the face","title":"faceSize"},{"location":"perceptors/peoplepom/#bodysize","text":"def bodySize(person_id) Returns the size of the body of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the body","title":"bodySize"},{"location":"perceptors/peoplepom/#wholesize","text":"def wholeSize(person_id) Returns the size of the whole person with the given id Arguments person_id ( str ): The person id Returns int : The size of the whole person","title":"wholeSize"},{"location":"perceptors/peoplepom/#bodypose","text":"def bodyPose(person_id) Returns the PoseNet data of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data","title":"bodyPose"},{"location":"perceptors/peoplepom/#bodyposehistory","text":"def bodyPoseHistory(person_id) Returns the PoseNet data history of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data","title":"bodyPoseHistory"},{"location":"perceptors/peoplepom/#travelpath","text":"def travelPath(person_id) Returns the travel path of the person with the given id Arguments person_id ( str ): The person id Returns The travel path","title":"travelPath"},{"location":"perceptors/peoplepom/#timeinview","text":"def timeInView(person_id) Returns the time the person with the given id has been in view Arguments person_id ( str ): The person id Returns int : The time in view","title":"timeInView"},{"location":"perceptors/peoplepom/#recentlydepartedpeople","text":"def recentlyDepartedPeople() Returns the people that have recently departed Returns list : The people that have recently departed","title":"recentlyDepartedPeople"},{"location":"perceptors/peoplepom/#occludedpeople","text":"def occludedPeople() Returns the people that are occluded Returns list : The people that are occluded","title":"occludedPeople"},{"location":"perceptors/peoplepom/#serialize","text":"def serialize() Returns the serialized data of the object Returns dict : The serialized data","title":"serialize"},{"location":"perceptors/perceptionobjectmodel/","text":"darcyai.perception_object_model PerceptionObjectModel Objects class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object. set_value def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set. get_perceptor def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception. get_perceptors def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors. serialize def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model. set_input_data def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data. get_input_data def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data. set_pulse_number def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number. get_pulse_number def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number. set_pps def set_pps(pps: int) -> None Set the pps for the perception object model. Arguments pps ( int ): The pps. get_pps def get_pps() -> int Get the pps for the perception object model. Returns int : The pps.","title":"PerceptionObjectModel"},{"location":"perceptors/perceptionobjectmodel/#darcyaiperception_object_model","text":"","title":"darcyai.perception_object_model"},{"location":"perceptors/perceptionobjectmodel/#perceptionobjectmodel-objects","text":"class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object.","title":"PerceptionObjectModel Objects"},{"location":"perceptors/perceptionobjectmodel/#set_value","text":"def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set.","title":"set_value"},{"location":"perceptors/perceptionobjectmodel/#get_perceptor","text":"def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception.","title":"get_perceptor"},{"location":"perceptors/perceptionobjectmodel/#get_perceptors","text":"def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors.","title":"get_perceptors"},{"location":"perceptors/perceptionobjectmodel/#serialize","text":"def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model.","title":"serialize"},{"location":"perceptors/perceptionobjectmodel/#set_input_data","text":"def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data.","title":"set_input_data"},{"location":"perceptors/perceptionobjectmodel/#get_input_data","text":"def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data.","title":"get_input_data"},{"location":"perceptors/perceptionobjectmodel/#set_pulse_number","text":"def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number.","title":"set_pulse_number"},{"location":"perceptors/perceptionobjectmodel/#get_pulse_number","text":"def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number.","title":"get_pulse_number"},{"location":"perceptors/perceptionobjectmodel/#set_pps","text":"def set_pps(pps: int) -> None Set the pps for the perception object model. Arguments pps ( int ): The pps.","title":"set_pps"},{"location":"perceptors/perceptionobjectmodel/#get_pps","text":"def get_pps() -> int Get the pps for the perception object model. Returns int : The pps.","title":"get_pps"},{"location":"perceptors/perceptor-utils/","text":"darcyai.perceptor.perceptor_utils get_supported_processors def get_supported_processors() -> dict Gets the list of supported processors. Returns dict - The list of supported processors. Examples >>> get_supported_processors() {<Processor.CORAL_EDGE_TPU: 1>: {'is_available': True, 'coral_tpus': [{'type': 'usb', 'path':\\ '/sys/bus/usb/devices/2-2'}]}, <Processor.CPU: 2>: {'is_available': True, 'cpu_count': 4}} get_perceptor_processor def get_perceptor_processor(processor_preference: list) -> Processor Gets the processor for the perceptor. Arguments processor_preference ( list ): The list of preferred processors. Returns Processor - The processor for the perceptor. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> get_perceptor_processor(processor_preference) Processor.CORAL_EDGE_TPU validate_processor_preference def validate_processor_preference(processor_preference, model_path_required=True) Validate the processor preference. Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, } >>> validate_processor_preference(processor_preference) validate_processor_list def validate_processor_list(processor_list) Validate the processor list. Arguments processor_list : A list of processors. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_list = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> validate_processor_list(processor_list)","title":"Perceptor Utils"},{"location":"perceptors/perceptor-utils/#darcyaiperceptorperceptor_utils","text":"","title":"darcyai.perceptor.perceptor_utils"},{"location":"perceptors/perceptor-utils/#get_supported_processors","text":"def get_supported_processors() -> dict Gets the list of supported processors. Returns dict - The list of supported processors. Examples >>> get_supported_processors() {<Processor.CORAL_EDGE_TPU: 1>: {'is_available': True, 'coral_tpus': [{'type': 'usb', 'path':\\ '/sys/bus/usb/devices/2-2'}]}, <Processor.CPU: 2>: {'is_available': True, 'cpu_count': 4}}","title":"get_supported_processors"},{"location":"perceptors/perceptor-utils/#get_perceptor_processor","text":"def get_perceptor_processor(processor_preference: list) -> Processor Gets the processor for the perceptor. Arguments processor_preference ( list ): The list of preferred processors. Returns Processor - The processor for the perceptor. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> get_perceptor_processor(processor_preference) Processor.CORAL_EDGE_TPU","title":"get_perceptor_processor"},{"location":"perceptors/perceptor-utils/#validate_processor_preference","text":"def validate_processor_preference(processor_preference, model_path_required=True) Validate the processor preference. Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, } >>> validate_processor_preference(processor_preference)","title":"validate_processor_preference"},{"location":"perceptors/perceptor-utils/#validate_processor_list","text":"def validate_processor_list(processor_list) Validate the processor list. Arguments processor_list : A list of processors. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_list = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> validate_processor_list(processor_list)","title":"validate_processor_list"},{"location":"perceptors/perceptor/","text":"darcyai.perceptor.perceptor Perceptor Objects class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass run def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor. load def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None . is_loaded def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise. set_loaded def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"Perceptor"},{"location":"perceptors/perceptor/#darcyaiperceptorperceptor","text":"","title":"darcyai.perceptor.perceptor"},{"location":"perceptors/perceptor/#perceptor-objects","text":"class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass","title":"Perceptor Objects"},{"location":"perceptors/perceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor.","title":"run"},{"location":"perceptors/perceptor/#load","text":"def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None .","title":"load"},{"location":"perceptors/perceptor/#is_loaded","text":"def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise.","title":"is_loaded"},{"location":"perceptors/perceptor/#set_loaded","text":"def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state.","title":"set_loaded"},{"location":"perceptors/perceptor/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"perceptors/perceptor/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"perceptors/perceptor/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"perceptors/coral-perceptors/coralperceptorbase/","text":"darcyai.perceptor.coral.coral_perceptor_base CoralPerceptorBase Objects class CoralPerceptorBase(Perceptor) Base class for all Coral Perceptors. load def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None . list_edge_tpus @staticmethod def list_edge_tpus() -> list Lists the Coral Edge TPUs. Returns list : The Coral Edge TPUs. Example >>> from darcyai.perceptor.coral.coral_perceptor_base import CoralPerceptorBase >>> CoralPerceptorBase.list_edge_tpus() [\"/device:coral:0\"]","title":"CoralPerceptorBase"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#darcyaiperceptorcoralcoral_perceptor_base","text":"","title":"darcyai.perceptor.coral.coral_perceptor_base"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#coralperceptorbase-objects","text":"class CoralPerceptorBase(Perceptor) Base class for all Coral Perceptors.","title":"CoralPerceptorBase Objects"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#load","text":"def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None .","title":"load"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#list_edge_tpus","text":"@staticmethod def list_edge_tpus() -> list Lists the Coral Edge TPUs. Returns list : The Coral Edge TPUs. Example >>> from darcyai.perceptor.coral.coral_perceptor_base import CoralPerceptorBase >>> CoralPerceptorBase.list_edge_tpus() [\"/device:coral:0\"]","title":"list_edge_tpus"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.coral.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. mean ( float ): The mean of the image. std ( float ): The standard deviation of the image. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns list[Class] : A list of detected classes. load def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use.","title":"ImageClassificationPerceptor"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#darcyaiperceptorcoralimage_classification_perceptor","text":"","title":"darcyai.perceptor.coral.image_classification_perceptor"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. mean ( float ): The mean of the image. std ( float ): The standard deviation of the image.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns list[Class] : A list of detected classes.","title":"run"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use.","title":"load"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.coral.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns list[Object] : A list of detected objects. load def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use. Returns None","title":"ObjectDetectionPerceptor"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#darcyaiperceptorcoralobject_detection_perceptor","text":"","title":"darcyai.perceptor.coral.object_detection_perceptor"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns list[Object] : A list of detected objects.","title":"run"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use. Returns None","title":"load"},{"location":"perceptors/coral-perceptors/peopleperceptor/","text":"darcyai.perceptor.coral.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(CoralPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/coral-perceptors/peopleperceptor/#darcyaiperceptorcoralpeople_perceptor","text":"","title":"darcyai.perceptor.coral.people_perceptor"},{"location":"perceptors/coral-perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(CoralPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/","text":"darcyai.perceptor.cpu.cpu_perceptor_base CpuPerceptorBase Objects class CpuPerceptorBase(Perceptor) Base class for all CPU Perceptors. load def load() -> None Loads the perceptor. read_label_file @staticmethod def read_label_file(filename: str, has_ids: bool = True, encoding: str = \"UTF-8\") -> dict Reads the labels file. Arguments filename ( str ): The path to the labels file. has_ids ( bool ): Whether the labels file contains IDs. encoding ( str ): The encoding of the labels file. Returns dict : A dictionary containing the labels.","title":"CpuPerceptorBase"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#darcyaiperceptorcpucpu_perceptor_base","text":"","title":"darcyai.perceptor.cpu.cpu_perceptor_base"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#cpuperceptorbase-objects","text":"class CpuPerceptorBase(Perceptor) Base class for all CPU Perceptors.","title":"CpuPerceptorBase Objects"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#load","text":"def load() -> None Loads the perceptor.","title":"load"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#read_label_file","text":"@staticmethod def read_label_file(filename: str, has_ids: bool = True, encoding: str = \"UTF-8\") -> dict Reads the labels file. Arguments filename ( str ): The path to the labels file. has_ids ( bool ): Whether the labels file contains IDs. encoding ( str ): The encoding of the labels file. Returns dict : A dictionary containing the labels.","title":"read_label_file"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.cpu.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(CpuPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns (list[Any], list(str)) : A tuple containing the detected classes and the labels. load def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): Not used.","title":"ImageClassificationPerceptor"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#darcyaiperceptorcpuimage_classification_perceptor","text":"","title":"darcyai.perceptor.cpu.image_classification_perceptor"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(CpuPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns (list[Any], list(str)) : A tuple containing the detected classes and the labels.","title":"run"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): Not used.","title":"load"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.cpu.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(CpuPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns (list[Any], list(str)) : A tuple containing the detected objects and the labels. load def load(accelerator_idx: [int, None] = None) -> None Loads the object detection model. Arguments accelerator_idx ( int ): Not used.","title":"ObjectDetectionPerceptor"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#darcyaiperceptorcpuobject_detection_perceptor","text":"","title":"darcyai.perceptor.cpu.object_detection_perceptor"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(CpuPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns (list[Any], list(str)) : A tuple containing the detected objects and the labels.","title":"run"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#load","text":"def load(accelerator_idx: [int, None] = None) -> None Loads the object detection model. Arguments accelerator_idx ( int ): Not used.","title":"load"},{"location":"perceptors/cpu-perceptors/peopleperceptor/","text":"darcyai.perceptor.cpu.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(CpuPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/cpu-perceptors/peopleperceptor/#darcyaiperceptorcpupeople_perceptor","text":"","title":"darcyai.perceptor.cpu.people_perceptor"},{"location":"perceptors/cpu-perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(CpuPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"pipeline-config/config/","text":"darcyai.config Config Objects class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. label ( str ): The label of the config. config_type ( str ): The type of the config. Valid types are: int float bool str rgb default_value ( Any ): The default value of the config. description ( str ): The description of the config. is_valid def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise. cast def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"Config"},{"location":"pipeline-config/config/#darcyaiconfig","text":"","title":"darcyai.config"},{"location":"pipeline-config/config/#config-objects","text":"class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. label ( str ): The label of the config. config_type ( str ): The type of the config. Valid types are: int float bool str rgb default_value ( Any ): The default value of the config. description ( str ): The description of the config.","title":"Config Objects"},{"location":"pipeline-config/config/#is_valid","text":"def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise.","title":"is_valid"},{"location":"pipeline-config/config/#cast","text":"def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"cast"},{"location":"pipeline-config/rgb/","text":"darcyai.config RGB Objects class RGB() Class to hold the configuration for the RGB. Arguments red ( int ): The red value. green ( int ): The green value. blue ( int ): The blue value. red def red() -> int Returns the red value. Returns int : The red value. green def green() -> int Returns the green value. Returns int : The green value. blue def blue() -> int Returns the blue value. Returns int : The blue value. to_hex def to_hex() -> str Returns the hex value of the RGB. Returns str : The hex value. from_string @staticmethod def from_string(rgb: str) -> \"RGB\" Creates an RGB object from a comma separated RGB string. Arguments rgb ( str ): The comma separated RGB string. Returns RGB : The RGB object. Examples >>> RGB.from_string(\"255,255,255\") from_hex_string @staticmethod def from_hex_string(hex_rgb: str) -> \"RGB\" Creates an RGB object from a hex RGB string. Arguments hex_rgb ( str ): The hex RGB string. Returns RGB : The RGB object.","title":"RGB"},{"location":"pipeline-config/rgb/#darcyaiconfig","text":"","title":"darcyai.config"},{"location":"pipeline-config/rgb/#rgb-objects","text":"class RGB() Class to hold the configuration for the RGB. Arguments red ( int ): The red value. green ( int ): The green value. blue ( int ): The blue value.","title":"RGB Objects"},{"location":"pipeline-config/rgb/#red","text":"def red() -> int Returns the red value. Returns int : The red value.","title":"red"},{"location":"pipeline-config/rgb/#green","text":"def green() -> int Returns the green value. Returns int : The green value.","title":"green"},{"location":"pipeline-config/rgb/#blue","text":"def blue() -> int Returns the blue value. Returns int : The blue value.","title":"blue"},{"location":"pipeline-config/rgb/#to_hex","text":"def to_hex() -> str Returns the hex value of the RGB. Returns str : The hex value.","title":"to_hex"},{"location":"pipeline-config/rgb/#from_string","text":"@staticmethod def from_string(rgb: str) -> \"RGB\" Creates an RGB object from a comma separated RGB string. Arguments rgb ( str ): The comma separated RGB string. Returns RGB : The RGB object. Examples >>> RGB.from_string(\"255,255,255\")","title":"from_string"},{"location":"pipeline-config/rgb/#from_hex_string","text":"@staticmethod def from_hex_string(hex_rgb: str) -> \"RGB\" Creates an RGB object from a hex RGB string. Arguments hex_rgb ( str ): The hex RGB string. Returns RGB : The RGB object.","title":"from_hex_string"}]}