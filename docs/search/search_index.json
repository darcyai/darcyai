{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Darcy AI SDK This is the official software development kit (SDK) for building on the Darcy AI platform. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 . Introducing the Darcy AI platform Darcy is an artificial intelligence (AI) that is focused on real-time interactions with the world. It has a variety of senses, such as vision and hearing, that allow it to perceive the local environment. You can give Darcy additional senses, such as LiDAR or thermal vision, to expand its capabilities. Darcy is present in every device where a Darcy AI application is running. It runs entirely in each device. No data needs to leave the device and no computation is done in the cloud. Darcy was designed to bring AI technology into the real world while keeping privacy and security as top priorities. Building real-time AI applications is very challenging. The Darcy AI SDK is a developer platform for computer vision and other applications which handles all of the most difficult and repetitive problems for you so you can focus on building amazing solutions. With the Darcy AI SDK, you get everything you need to build real-time AI applications. If you can write web applications with Node.js or you have moderate Python experience, then you can develop a fully functioning AI app with Darcy. The SDK comes with documentation, build instructions, example applications, and more. How to use this SDK Use this document and the other markdown files in this directory to learn about the Darcy AI platform and to get started. The guide documents will take you from an absolute beginner to building and deploying your own Darcy AI applications. The docs folder contains the full technical Darcy AI programming documentation. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 . What you need To develop and package Darcy AI applications, you only need a Mac, Windows, or Linux computer. To run Darcy AI applications on devices, you just need one or more devices that meet the minimum requirements below. System requirements for developer environments Any modern CPU (both x86 and ARM processors will work) 4GB of RAM Camera (the integrated webcam on most laptops will work) 5GB of available disk space for Docker, Python, required libraries, and your application containers Optional: Google Coral AI accelerator for testing accelerated processing System requirements for running deployed Darcy AI applications ARM or x86 CPU (two or more cores recommended) Google Coral AI accelerator (more than one Coral increases performance for many applications) 512MB of RAM (4GB or more recommended) Camera (required for using Darcy with live video) Internet connectivity (wired Ethernet or WiFi) 200MB available disk space (1GB or more recommended and your application size will vary) Darcy AI ready edge boards Raspberry Pi with Coral USB Raspberry Pi 4 single board computer https://www.raspberrypi.com/products/raspberry-pi-4-model-b/ Google Coral AI accelerator USB module https://coral.ai/products/accelerator/ Nvidia Jetson Nano with Coral USB Jetson Nano Developer Kit https://developer.nvidia.com/embedded/jetson-nano-developer-kit Google Coral AI accelerator USB module https://coral.ai/products/accelerator/ Google Coral Dev Board https://coral.ai/products/dev-board/ Google Coral Dev Board Mini https://coral.ai/products/dev-board-mini/ ASUS Tinker T https://www.asus.com/us/Networking-IoT-Servers/AIoT-Industrial-Solutions/Tinker-Board-Series/Tinker-Edge-T/ Getting started If you haven't already become familiar with the Darcy AI platform terminology, check out this Terminology Guide to get up to speed quickly. The best way to get started with Darcy is to see it in action. Start by trying out the Darcy AI Explorer application in the Darcy Explorer Guide . When you are ready to create, you can launch your Darcy AI developer journey with the Getting Started Guide . Building To prepare your Mac OS computer as a development environment for building Darcy AI applications, follow the Mac OS X Environment Setup Guide . To prepare your Windows computer as a development environment for building Darcy AI applications, follow the Windows Environment Setup Guide . Learn how to build and run a Darcy AI application using your favorite IDE with the Build Guide . Packaging Learn how to package your Darcy AI application into a container that includes all of the dependencies needed to run on many different devices with the Packaging Guide . Deploying Learn how to deploy your Darcy AI application to your edge devices using the Darcy Cloud in the Deployment Guide . Documentation Open the Darcy AI technical documentation to search and browse the API with code examples. This is a local documentation site that will run directly in your browser. The documentation is specific to each version of the Darcy AI SDK so it's the best place to reference when building. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 . If you prefer to access the latest Darcy AI developer documentation with an internet connection, use Hosted Darcy AI Documentation . Resources Examples The examples folder contains a diverse set of sample applications that you can use as a reference or as a start of your own Darcy AI application. The code is commented to help you understand what to do and when to do it. Here are some short descriptions to help you understand what examples are available. Some example applications are stored in their own code repositories to make learning and building easier. Darcy AI Explorer - this demo application is a rich showcase of what the Darcy AI system can do. Use the source code as a model for building a full-featured production application. https://github.com/darcyai/darcyai-explorer Real-time Audio Analysis - Build and deploy this Darcy AI demo application to learn how to add audio capabilities to Darcy and listen for important sounds. Audio Analysis Basic Darcy AI Pipeline - Use this demo application to learn the basics of creating a Darcy AI Pipeline. Basic Pipeline Sample Output Stream - Learn how to create a Darcy AI Output Stream by adding this example to your application. The example sends your Darcy AI output to an Amazon Web Services S3 bucket. Output Stream Sample Perceptors - Learn how to build your own Darcy AI Perceptors with the examples in this directory. There is a basic mock perceptor that you can use as a template. There is a face detector perceptor that uses an AI model to find faces. There is also a face mask detector perceptor that checks a person's face for a mask. Perceptors Heart Rate Demo - this demo application is a good example of how to build an edge application that is made of multiple microservices that communicate with each other. https://github.com/darcyai/heart-rate-demo Sample Build Files - Get a sample Dockerfile in this folder so you don't have to create one from scratch. Build Files Sample Deployment Files - Get a sample application YAML file in this folder so you can just replace the default values and deploy your edge AI application easily. Deployment Files Getting help Get help from the other Darcy AI developers, the Darcy team, and the whole Darcy community on the Darcy AI Forum at https://discuss.darcy.ai/c/darcy-ai/ Report and track bugs in the Darcy AI platform and SDK using Github issues https://github.com/darcyai/darcyai/issues Python packages for Darcy AI Darcy AI package https://pypi.org/project/darcyai/ Darcy Cloud Deploy and manage edge applications including Darcy AI applications with the Darcy Cloud. Create an account for free at https://cloud.darcy.ai Other helpful links Company website for Edgeworx, the providers of the Darcy AI platform https://darcy.ai Official website for the Tensorflow AI project https://www.tensorflow.org","title":"Home"},{"location":"#darcy-ai-sdk","text":"This is the official software development kit (SDK) for building on the Darcy AI platform. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 .","title":"Darcy AI SDK"},{"location":"#introducing-the-darcy-ai-platform","text":"Darcy is an artificial intelligence (AI) that is focused on real-time interactions with the world. It has a variety of senses, such as vision and hearing, that allow it to perceive the local environment. You can give Darcy additional senses, such as LiDAR or thermal vision, to expand its capabilities. Darcy is present in every device where a Darcy AI application is running. It runs entirely in each device. No data needs to leave the device and no computation is done in the cloud. Darcy was designed to bring AI technology into the real world while keeping privacy and security as top priorities. Building real-time AI applications is very challenging. The Darcy AI SDK is a developer platform for computer vision and other applications which handles all of the most difficult and repetitive problems for you so you can focus on building amazing solutions. With the Darcy AI SDK, you get everything you need to build real-time AI applications. If you can write web applications with Node.js or you have moderate Python experience, then you can develop a fully functioning AI app with Darcy. The SDK comes with documentation, build instructions, example applications, and more.","title":"Introducing the Darcy AI platform"},{"location":"#how-to-use-this-sdk","text":"Use this document and the other markdown files in this directory to learn about the Darcy AI platform and to get started. The guide documents will take you from an absolute beginner to building and deploying your own Darcy AI applications. The docs folder contains the full technical Darcy AI programming documentation. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 .","title":"How to use this SDK"},{"location":"#what-you-need","text":"To develop and package Darcy AI applications, you only need a Mac, Windows, or Linux computer. To run Darcy AI applications on devices, you just need one or more devices that meet the minimum requirements below.","title":"What you need"},{"location":"#system-requirements-for-developer-environments","text":"Any modern CPU (both x86 and ARM processors will work) 4GB of RAM Camera (the integrated webcam on most laptops will work) 5GB of available disk space for Docker, Python, required libraries, and your application containers Optional: Google Coral AI accelerator for testing accelerated processing","title":"System requirements for developer environments"},{"location":"#system-requirements-for-running-deployed-darcy-ai-applications","text":"ARM or x86 CPU (two or more cores recommended) Google Coral AI accelerator (more than one Coral increases performance for many applications) 512MB of RAM (4GB or more recommended) Camera (required for using Darcy with live video) Internet connectivity (wired Ethernet or WiFi) 200MB available disk space (1GB or more recommended and your application size will vary)","title":"System requirements for running deployed Darcy AI applications"},{"location":"#darcy-ai-ready-edge-boards","text":"Raspberry Pi with Coral USB Raspberry Pi 4 single board computer https://www.raspberrypi.com/products/raspberry-pi-4-model-b/ Google Coral AI accelerator USB module https://coral.ai/products/accelerator/ Nvidia Jetson Nano with Coral USB Jetson Nano Developer Kit https://developer.nvidia.com/embedded/jetson-nano-developer-kit Google Coral AI accelerator USB module https://coral.ai/products/accelerator/ Google Coral Dev Board https://coral.ai/products/dev-board/ Google Coral Dev Board Mini https://coral.ai/products/dev-board-mini/ ASUS Tinker T https://www.asus.com/us/Networking-IoT-Servers/AIoT-Industrial-Solutions/Tinker-Board-Series/Tinker-Edge-T/","title":"Darcy AI ready edge boards"},{"location":"#getting-started","text":"If you haven't already become familiar with the Darcy AI platform terminology, check out this Terminology Guide to get up to speed quickly. The best way to get started with Darcy is to see it in action. Start by trying out the Darcy AI Explorer application in the Darcy Explorer Guide . When you are ready to create, you can launch your Darcy AI developer journey with the Getting Started Guide .","title":"Getting started"},{"location":"#building","text":"To prepare your Mac OS computer as a development environment for building Darcy AI applications, follow the Mac OS X Environment Setup Guide . To prepare your Windows computer as a development environment for building Darcy AI applications, follow the Windows Environment Setup Guide . Learn how to build and run a Darcy AI application using your favorite IDE with the Build Guide .","title":"Building"},{"location":"#packaging","text":"Learn how to package your Darcy AI application into a container that includes all of the dependencies needed to run on many different devices with the Packaging Guide .","title":"Packaging"},{"location":"#deploying","text":"Learn how to deploy your Darcy AI application to your edge devices using the Darcy Cloud in the Deployment Guide .","title":"Deploying"},{"location":"#documentation","text":"Open the Darcy AI technical documentation to search and browse the API with code examples. This is a local documentation site that will run directly in your browser. The documentation is specific to each version of the Darcy AI SDK so it's the best place to reference when building. To browse this document and all of the other documentation on a local web server, run the apidocs.bash script and then visit http://localhost:8000 . If you prefer to access the latest Darcy AI developer documentation with an internet connection, use Hosted Darcy AI Documentation .","title":"Documentation"},{"location":"#resources","text":"","title":"Resources"},{"location":"#examples","text":"The examples folder contains a diverse set of sample applications that you can use as a reference or as a start of your own Darcy AI application. The code is commented to help you understand what to do and when to do it. Here are some short descriptions to help you understand what examples are available. Some example applications are stored in their own code repositories to make learning and building easier. Darcy AI Explorer - this demo application is a rich showcase of what the Darcy AI system can do. Use the source code as a model for building a full-featured production application. https://github.com/darcyai/darcyai-explorer Real-time Audio Analysis - Build and deploy this Darcy AI demo application to learn how to add audio capabilities to Darcy and listen for important sounds. Audio Analysis Basic Darcy AI Pipeline - Use this demo application to learn the basics of creating a Darcy AI Pipeline. Basic Pipeline Sample Output Stream - Learn how to create a Darcy AI Output Stream by adding this example to your application. The example sends your Darcy AI output to an Amazon Web Services S3 bucket. Output Stream Sample Perceptors - Learn how to build your own Darcy AI Perceptors with the examples in this directory. There is a basic mock perceptor that you can use as a template. There is a face detector perceptor that uses an AI model to find faces. There is also a face mask detector perceptor that checks a person's face for a mask. Perceptors Heart Rate Demo - this demo application is a good example of how to build an edge application that is made of multiple microservices that communicate with each other. https://github.com/darcyai/heart-rate-demo Sample Build Files - Get a sample Dockerfile in this folder so you don't have to create one from scratch. Build Files Sample Deployment Files - Get a sample application YAML file in this folder so you can just replace the default values and deploy your edge AI application easily. Deployment Files","title":"Examples"},{"location":"#getting-help","text":"Get help from the other Darcy AI developers, the Darcy team, and the whole Darcy community on the Darcy AI Forum at https://discuss.darcy.ai/c/darcy-ai/ Report and track bugs in the Darcy AI platform and SDK using Github issues https://github.com/darcyai/darcyai/issues","title":"Getting help"},{"location":"#python-packages-for-darcy-ai","text":"Darcy AI package https://pypi.org/project/darcyai/","title":"Python packages for Darcy AI"},{"location":"#darcy-cloud","text":"Deploy and manage edge applications including Darcy AI applications with the Darcy Cloud. Create an account for free at https://cloud.darcy.ai","title":"Darcy Cloud"},{"location":"#other-helpful-links","text":"Company website for Edgeworx, the providers of the Darcy AI platform https://darcy.ai Official website for the Tensorflow AI project https://www.tensorflow.org","title":"Other helpful links"},{"location":"build/","text":"Building your Darcy AI application It\u2019s easy to build a Darcy AI application but how do you get started? Here\u2019s an example application that introduces all of the main concepts you will need for building your own application. Start by following along! What you will accomplish This guide will walk you through the creation of a simple yet powerful Darcy AI application. Many of the impressive features of Darcy AI are used here in this simple demo app so you can immediately gain experience working with those features. By the end of this guide you will have built and run your first Darcy AI application in your favorite IDE. After you have successfully accomplished everything in this guide, you should be able to modify the code to customize the application or begin building your own Darcy AI application with a similar structure and level of complexity. Follow the next step recommended at the bottom of this guide to learn how to package and then deploy your Darcy AI applications. After those steps, you will be ready to learn more advanced Darcy AI app development. Requirements You\u2019ll need to have a few things in place before you build. Here\u2019s the list: - A laptop or desktop computer running Mac OS X or Windows - A webcam or USB camera (the built-in webcam on your computer should work nicely) - Python version 3.6.9 or greater - An integrated development environment (IDE) for writing and debugging your code - The Darcy AI Python library and other supporting Python packages - Docker for Mac, Windows, or Linux depending on your computer When you are ready to package and deploy your Darcy AI application, try any of the following: - A Raspberry Pi with an attached video camera (and an optional Google Coral edge TPU for dramatically increased performance) - An Nvidia Jetson Nano with an attached video camera (and an optional Google Coral edge TPU will increase performance here, too) - An Intel NUC edge computer with a USB camera - Any other edge compute board that can receive camera input and runs the Linux operating system Environment setup If you are using a MacOS laptop or desktop, follow the Mac OS X Environment Setup Guide . If you are using a Windows laptop or desktop, follow the Windows Environment Setup Guide . You can also use an edge compute board as your development environment. Choose from the following options to set up your edge board instead of your laptop or desktop computer. You do not need to follow these environment setup steps for a Raspberry Pi or Jetson Nano board if you are just using those boards to run your packaged Darcy AI applications. If you need to setup a Raspberry Pi as a Darcy AI development environment, follow the Raspberry Pi Environment Setup Guide . If you need to setup a Jetson Nano as a Darcy AI development environment, follow the Jetson Nano Environment Setup Guide . Create your application Python file and import libraries You only need a single Python file to build a Darcy AI application. Open a new .py file in your favorite IDE and name it whatever you want. Then add the following statements at the top to include the Darcy AI libraries and some additional helpful libraries: import cv2 import os import pathlib from darcyai.perceptor.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline from darcyai.config import RGB If you don\u2019t have the darcyai library installed yet, you can install it with PIP package installer for Python using the following commands: pip install darcyai If you have multiple versions of Python on your system, you may need to install the darcyai library using the Python3 version of PIP as follows: pip3 install darcyai Add the Pipeline, Input Stream, and Output Stream objects This part is quite easy. Just follow the comments to learn more about these 3 important lines of code. # Instantiate an Camera Stream input stream object camera = CameraStream(video_device=0, fps=20) # Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) # Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") Set up a callback and add the Output Stream to the Pipeline Before we add the LiveFeed Output Stream to the Pipeline, we need to set up a callback function that we are going to use to process the data before displaying the video. Follow the comments to learn about the steps that are taken. This is the most complex portion of the whole application and it is where all of the business logic is taking place. After the callback function definition, there is a line for adding the LiveFeed Output Stream to the Pipeline. That command needs to have the callback function already defined before it can execute successfully. # Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): # Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() # Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # If we have anyone, demonstrate looking up that person in the POM by getting their face size # And then put it on the frame as some text # NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # Pass the finished frame out of this callback so the Live Feed output stream can display it return frame # Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) Define an event Output Stream and an input Output Stream and instantiate the People Perceptor Just like the LiveFeed Output Stream, the People Perceptor must have the callback already defined before it can work with those callbacks. The input callback simply takes the Input Stream data and sends it onward to the People Perceptor . The \u201cNew Person\u201d event callback simply prints the unique person identifier string to the console output when a new person has been detected by Darcy AI. # Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): # Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame # Create a callback function for handling the \"New Person\" event from the People Perceptor # Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) # Instantiate a People Perceptor people_ai = PeoplePerceptor() # Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) Add the People Perceptor to the Pipeline # Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) Change some configuration items in the People Perceptor # Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", RGB(0, 255, 0)) Start the Pipeline # Start the Pipeline pipeline.run() Check your completed code Your finished Python file should look similar to this. If it doesn\u2019t, take a minute to figure out what is missing or incorrect. Save your Python file. Next we will run your code! import cv2 import os import pathlib from darcyai.perceptor.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline from darcyai.config import RGB # Instantiate an Camera Stream input stream object camera = CameraStream(video_device=0, fps=20) # Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) # Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") # Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): # Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() # Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # If we have anyone, demonstrate looking up that person in the POM by getting their face size # And then put it on the frame as some text # NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # Pass the finished frame out of this callback so the Live Feed output stream can display it return frame # Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) # Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): # Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame # Create a callback function for handling the \"New Person\" event from the People Perceptor # Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) # Instantiate a People Perceptor people_ai = PeoplePerceptor() # Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) # Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) # Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", RGB(0, 255, 0)) # Start the Pipeline pipeline.run() Run your application Using your IDE, run your Python code. Don't set any breakpoints at first because that will prevent you from seeing the video stream. If you followed the code reference above directly and you have all of the required Python libraries installed, your Darcy AI application should run successfully and stay running until you stop the program execution. View your real-time Darcy AI application video output Once your application is running, you can view the live video feed by visiting the following URL in any browser. The port number 3456 has been specified in the Python code. Feel free to change it and use the alternate port in the URL below. http://localhost:3456/ What you should see You should see a live video feed coming from your camera. When a person is detected in the field of view, some information should be displayed on the video and some dots should be drawn on top of key face locations. The dots should move with the person's face. This is a demonstration of using Darcy AI to detect the presence of people, assign an anonymous stable identifier to persons as they move around the field of view, and annotate the video frames with text and graphics. Now package your Darcy AI application for deployment Now that your Darcy AI application is working, the next step is to learn how to package it for deployment to a wide range of devices! Follow the Packaging Guide to learn how to package your Darcy AI apps.","title":"Build"},{"location":"build/#building-your-darcy-ai-application","text":"It\u2019s easy to build a Darcy AI application but how do you get started? Here\u2019s an example application that introduces all of the main concepts you will need for building your own application. Start by following along!","title":"Building your Darcy AI application"},{"location":"build/#what-you-will-accomplish","text":"This guide will walk you through the creation of a simple yet powerful Darcy AI application. Many of the impressive features of Darcy AI are used here in this simple demo app so you can immediately gain experience working with those features. By the end of this guide you will have built and run your first Darcy AI application in your favorite IDE. After you have successfully accomplished everything in this guide, you should be able to modify the code to customize the application or begin building your own Darcy AI application with a similar structure and level of complexity. Follow the next step recommended at the bottom of this guide to learn how to package and then deploy your Darcy AI applications. After those steps, you will be ready to learn more advanced Darcy AI app development.","title":"What you will accomplish"},{"location":"build/#requirements","text":"You\u2019ll need to have a few things in place before you build. Here\u2019s the list: - A laptop or desktop computer running Mac OS X or Windows - A webcam or USB camera (the built-in webcam on your computer should work nicely) - Python version 3.6.9 or greater - An integrated development environment (IDE) for writing and debugging your code - The Darcy AI Python library and other supporting Python packages - Docker for Mac, Windows, or Linux depending on your computer When you are ready to package and deploy your Darcy AI application, try any of the following: - A Raspberry Pi with an attached video camera (and an optional Google Coral edge TPU for dramatically increased performance) - An Nvidia Jetson Nano with an attached video camera (and an optional Google Coral edge TPU will increase performance here, too) - An Intel NUC edge computer with a USB camera - Any other edge compute board that can receive camera input and runs the Linux operating system","title":"Requirements"},{"location":"build/#environment-setup","text":"If you are using a MacOS laptop or desktop, follow the Mac OS X Environment Setup Guide . If you are using a Windows laptop or desktop, follow the Windows Environment Setup Guide . You can also use an edge compute board as your development environment. Choose from the following options to set up your edge board instead of your laptop or desktop computer. You do not need to follow these environment setup steps for a Raspberry Pi or Jetson Nano board if you are just using those boards to run your packaged Darcy AI applications. If you need to setup a Raspberry Pi as a Darcy AI development environment, follow the Raspberry Pi Environment Setup Guide . If you need to setup a Jetson Nano as a Darcy AI development environment, follow the Jetson Nano Environment Setup Guide .","title":"Environment setup"},{"location":"build/#create-your-application-python-file-and-import-libraries","text":"You only need a single Python file to build a Darcy AI application. Open a new .py file in your favorite IDE and name it whatever you want. Then add the following statements at the top to include the Darcy AI libraries and some additional helpful libraries: import cv2 import os import pathlib from darcyai.perceptor.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline from darcyai.config import RGB If you don\u2019t have the darcyai library installed yet, you can install it with PIP package installer for Python using the following commands: pip install darcyai If you have multiple versions of Python on your system, you may need to install the darcyai library using the Python3 version of PIP as follows: pip3 install darcyai","title":"Create your application Python file and import libraries"},{"location":"build/#add-the-pipeline-input-stream-and-output-stream-objects","text":"This part is quite easy. Just follow the comments to learn more about these 3 important lines of code. # Instantiate an Camera Stream input stream object camera = CameraStream(video_device=0, fps=20) # Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) # Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\")","title":"Add the Pipeline, Input Stream, and Output Stream objects"},{"location":"build/#set-up-a-callback-and-add-the-output-stream-to-the-pipeline","text":"Before we add the LiveFeed Output Stream to the Pipeline, we need to set up a callback function that we are going to use to process the data before displaying the video. Follow the comments to learn about the steps that are taken. This is the most complex portion of the whole application and it is where all of the business logic is taking place. After the callback function definition, there is a line for adding the LiveFeed Output Stream to the Pipeline. That command needs to have the callback function already defined before it can execute successfully. # Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): # Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() # Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # If we have anyone, demonstrate looking up that person in the POM by getting their face size # And then put it on the frame as some text # NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # Pass the finished frame out of this callback so the Live Feed output stream can display it return frame # Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed)","title":"Set up a callback and add the Output Stream to the Pipeline"},{"location":"build/#define-an-event-output-stream-and-an-input-output-stream-and-instantiate-the-people-perceptor","text":"Just like the LiveFeed Output Stream, the People Perceptor must have the callback already defined before it can work with those callbacks. The input callback simply takes the Input Stream data and sends it onward to the People Perceptor . The \u201cNew Person\u201d event callback simply prints the unique person identifier string to the console output when a new person has been detected by Darcy AI. # Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): # Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame # Create a callback function for handling the \"New Person\" event from the People Perceptor # Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) # Instantiate a People Perceptor people_ai = PeoplePerceptor() # Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback)","title":"Define an event Output Stream and an input Output Stream and instantiate the People Perceptor"},{"location":"build/#add-the-people-perceptor-to-the-pipeline","text":"# Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback)","title":"Add the People Perceptor to the Pipeline"},{"location":"build/#change-some-configuration-items-in-the-people-perceptor","text":"# Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", RGB(0, 255, 0))","title":"Change some configuration items in the People Perceptor"},{"location":"build/#start-the-pipeline","text":"# Start the Pipeline pipeline.run()","title":"Start the Pipeline"},{"location":"build/#check-your-completed-code","text":"Your finished Python file should look similar to this. If it doesn\u2019t, take a minute to figure out what is missing or incorrect. Save your Python file. Next we will run your code! import cv2 import os import pathlib from darcyai.perceptor.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline from darcyai.config import RGB # Instantiate an Camera Stream input stream object camera = CameraStream(video_device=0, fps=20) # Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) # Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") # Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): # Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() # Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # If we have anyone, demonstrate looking up that person in the POM by getting their face size # And then put it on the frame as some text # NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) # Pass the finished frame out of this callback so the Live Feed output stream can display it return frame # Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) # Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): # Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame # Create a callback function for handling the \"New Person\" event from the People Perceptor # Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) # Instantiate a People Perceptor people_ai = PeoplePerceptor() # Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) # Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) # Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", RGB(0, 255, 0)) # Start the Pipeline pipeline.run()","title":"Check your completed code"},{"location":"build/#run-your-application","text":"Using your IDE, run your Python code. Don't set any breakpoints at first because that will prevent you from seeing the video stream. If you followed the code reference above directly and you have all of the required Python libraries installed, your Darcy AI application should run successfully and stay running until you stop the program execution.","title":"Run your application"},{"location":"build/#view-your-real-time-darcy-ai-application-video-output","text":"Once your application is running, you can view the live video feed by visiting the following URL in any browser. The port number 3456 has been specified in the Python code. Feel free to change it and use the alternate port in the URL below. http://localhost:3456/","title":"View your real-time Darcy AI application video output"},{"location":"build/#what-you-should-see","text":"You should see a live video feed coming from your camera. When a person is detected in the field of view, some information should be displayed on the video and some dots should be drawn on top of key face locations. The dots should move with the person's face. This is a demonstration of using Darcy AI to detect the presence of people, assign an anonymous stable identifier to persons as they move around the field of view, and annotate the video frames with text and graphics.","title":"What you should see"},{"location":"build/#now-package-your-darcy-ai-application-for-deployment","text":"Now that your Darcy AI application is working, the next step is to learn how to package it for deployment to a wide range of devices! Follow the Packaging Guide to learn how to package your Darcy AI apps.","title":"Now package your Darcy AI application for deployment"},{"location":"darcy-ai-explorer/","text":"Darcy AI Explorer Guide The Darcy AI Explorer application is a great way to experience Darcy for the first time. This sample application is rich in features so you can get a sense for what Darcy provides you as a developer. Follow the steps in this guide to get your device connected to the Darcy Cloud and deploy the Darcy AI Explorer application. About the Darcy Cloud The Darcy Cloud gives you management of all your edge devices and edge applications in one place. You can open an SSH shell session on demand, deploy applications, and see the health and status for every device. All of this functionality works no matter where your edge devices are physically located, even when they are behind NAT layers and firewalls. Use the Darcy Cloud to make building, deploying, and debugging easier, and then use it to operate your edge AI applications in production systems. Connect your device to the Darcy Cloud If you don't already have an account, you can create one now for free. Create an account or log in at https://cloud.darcy.ai . Once you are in your Darcy Cloud account, add your device as a node in your current project. Use the \"plus button\" in the bottom left to add a node to your project. Follow the instructions in the pop-up window to add your device as a node. Deploy the Darcy AI Explorer application Click on the \"plus button\" and choose \"app\" to deploy a new application to your device. In the pop-up choose the \"Darcy AI Explorer App\" and then click \"Next\". Choose your device from the drop-down menu and then click \"Deploy\". The Darcy AI Explorer application will begin to download to your device. You can track the status of the app in the Darcy Cloud UI. When the app is listed as \"Running\" you can proceed to the next step. Depending on the Internet connection speed of your device, it may take about 15 minutes for the Darcy AI Explorer app to download and start on your device. Open the Darcy AI Explorer Once your Darcy AI Explorer app is running, you can view the UI and use the app by visiting the following URL in any browser. Replace YOUR.DEVICE.IP.ADDRESS with the actual IP address of your device. See Darcy AI in action and explore what it can do! http://YOUR.DEVICE.IP.ADDRESS:5555/ Continue your Darcy AI exploration Now that you have a running Darcy AI application to explore, continue your journey by checking out the examples and getting started building your own application! Darcy AI Documentation Home","title":"Darcy AI Explorer"},{"location":"darcy-ai-explorer/#darcy-ai-explorer-guide","text":"The Darcy AI Explorer application is a great way to experience Darcy for the first time. This sample application is rich in features so you can get a sense for what Darcy provides you as a developer. Follow the steps in this guide to get your device connected to the Darcy Cloud and deploy the Darcy AI Explorer application.","title":"Darcy AI Explorer Guide"},{"location":"darcy-ai-explorer/#about-the-darcy-cloud","text":"The Darcy Cloud gives you management of all your edge devices and edge applications in one place. You can open an SSH shell session on demand, deploy applications, and see the health and status for every device. All of this functionality works no matter where your edge devices are physically located, even when they are behind NAT layers and firewalls. Use the Darcy Cloud to make building, deploying, and debugging easier, and then use it to operate your edge AI applications in production systems.","title":"About the Darcy Cloud"},{"location":"darcy-ai-explorer/#connect-your-device-to-the-darcy-cloud","text":"If you don't already have an account, you can create one now for free. Create an account or log in at https://cloud.darcy.ai . Once you are in your Darcy Cloud account, add your device as a node in your current project. Use the \"plus button\" in the bottom left to add a node to your project. Follow the instructions in the pop-up window to add your device as a node.","title":"Connect your device to the Darcy Cloud"},{"location":"darcy-ai-explorer/#deploy-the-darcy-ai-explorer-application","text":"Click on the \"plus button\" and choose \"app\" to deploy a new application to your device. In the pop-up choose the \"Darcy AI Explorer App\" and then click \"Next\". Choose your device from the drop-down menu and then click \"Deploy\". The Darcy AI Explorer application will begin to download to your device. You can track the status of the app in the Darcy Cloud UI. When the app is listed as \"Running\" you can proceed to the next step. Depending on the Internet connection speed of your device, it may take about 15 minutes for the Darcy AI Explorer app to download and start on your device.","title":"Deploy the Darcy AI Explorer application"},{"location":"darcy-ai-explorer/#open-the-darcy-ai-explorer","text":"Once your Darcy AI Explorer app is running, you can view the UI and use the app by visiting the following URL in any browser. Replace YOUR.DEVICE.IP.ADDRESS with the actual IP address of your device. See Darcy AI in action and explore what it can do! http://YOUR.DEVICE.IP.ADDRESS:5555/","title":"Open the Darcy AI Explorer"},{"location":"darcy-ai-explorer/#continue-your-darcy-ai-exploration","text":"Now that you have a running Darcy AI application to explore, continue your journey by checking out the examples and getting started building your own application! Darcy AI Documentation Home","title":"Continue your Darcy AI exploration"},{"location":"deploy/","text":"Darcy AI Application Deployment Guide Once you have built and packaged a Darcy AI application, you can deploy it to as many devices as you want. Use this guide to get devices set up on the Darcy Cloud and create your own deployment YAML files and deploy your applications. Make sure your Darcy AI application container is available You should have completed the steps in the Packaging Guide by now. If have not, follow that guide now to package your Darcy AI application. In the packaging process you specified a full application container identifier for your Darcy AI application. This identifier consists of an organization name followed by a / and then a container name followed by a : and then a tag. As an example, the identifier darcyai/darcy-ai-explorer:1.0.2 represents an application called darcy-ai-explorer with a tag 1.0.2 that is hosted under the Docker Hub organization darcyai . You will use your container identifier in your application deployment YAML file below. Make sure your container images were successfully pushed to Docker Hub at the conclusion of your packaging process. About the Darcy Cloud The Darcy Cloud gives you management of all your edge devices and edge applications in one place. You can open an SSH shell session on demand, deploy applications, and see the health and status for every device. All of this functionality works no matter where your edge devices are physically located, even when they are behind NAT layers and firewalls. Use the Darcy Cloud to make building, deploying, and debugging easier, and then use it to operate your edge AI applications in production systems. Add your devices to the Darcy Cloud If you don't already have an account, you can create one now for free. Create an account or log in at https://cloud.darcy.ai . Once you are in your Darcy Cloud account, add your device as a node in your current project. Use the \"plus button\" in the bottom left to add a node. Follow the instructions in the pop-up window to add your device as a node. Create your application YAML Here is a sample YAML file to work with. kind: Application apiVersion: iofog.org/v3 metadata: name: your-application-name spec: microservices: - name: your-microservice-name agent: name: your-darcy-cloud-node-name images: arm: 'YOUR_ORGANIZATION/YOUR_APP:tag.goes.here' x86: 'YOUR_ORGANIZATION/YOUR_APP:tag.goes.here' container: rootHostAccess: true ports: [] volumes: - containerDestination: /dev hostDestination: /dev type: bind accessMode: rw You can find this sample YAMl file in the examples/deploy/ directory called app_deployment.yml . Your application deployment YAML file contains the information that the Darcy Cloud uses to load and run your Darcy AI application on any device. Replace the placeholder fields with your own information and save the file with whatever file name you like, such as my-app-deploy.yml . For the agent name, which is shown above as your-darcy-cloud-node-name you should use the actual node name from your Darcy Cloud account. This is the name that shows for your device which you added in the steps above. Deploy your Darcy AI application Now that you have all of the pieces, it's easy to deploy your application to your device or any other device. In the Darcy Cloud, click on the \"plus button\" in the bottom left and choose \"app\". In the pop-up window, choose the \"upload your app\" option and you will see a drag-and-drop window on the right-hand side. You can drag and drop your YAML file into that window or you can click the \"browse and upload\" option and then select your YAML file. The Darcy Cloud will tell you if you have any issues with your YAML file or your app deployment. It will also tell you if your Darcy AI application was deployed successfully. You can then check the status of your application using the Darcy Cloud. Use your Darcy AI application When your Darcy AI application has successfully been deployed to your devices, you will see the status running in your Darcy Cloud UI. At this time, your Darcy AI application is fully running on those devices. If your application has a live video feed, such as the demo application you built in the Build Guide at port 3456 then you should be able to view the live feed using the IP address of the device followed by :3456 . Replace the IP address in the example below with your device's IP address to view the live feed. http://192.168.1.20:3456 You have accomplished a great amount at this point. Congratulations! You have developed a Darcy AI application and tested it with your IDE and local development environment. You have packaged your application for a variety of target devices. And you have made a deployment YAML file and used the Darcy Cloud to deploy and manage your Darcy AI appliation. Wow! Next steps Now that you have all of these foundation Darcy AI developer skills, you are ready to build full solutions. Use the Technical Documentation to learn more about what Darcy AI can do and take your skills to the next level.","title":"Deploy"},{"location":"deploy/#darcy-ai-application-deployment-guide","text":"Once you have built and packaged a Darcy AI application, you can deploy it to as many devices as you want. Use this guide to get devices set up on the Darcy Cloud and create your own deployment YAML files and deploy your applications.","title":"Darcy AI Application Deployment Guide"},{"location":"deploy/#make-sure-your-darcy-ai-application-container-is-available","text":"You should have completed the steps in the Packaging Guide by now. If have not, follow that guide now to package your Darcy AI application. In the packaging process you specified a full application container identifier for your Darcy AI application. This identifier consists of an organization name followed by a / and then a container name followed by a : and then a tag. As an example, the identifier darcyai/darcy-ai-explorer:1.0.2 represents an application called darcy-ai-explorer with a tag 1.0.2 that is hosted under the Docker Hub organization darcyai . You will use your container identifier in your application deployment YAML file below. Make sure your container images were successfully pushed to Docker Hub at the conclusion of your packaging process.","title":"Make sure your Darcy AI application container is available"},{"location":"deploy/#about-the-darcy-cloud","text":"The Darcy Cloud gives you management of all your edge devices and edge applications in one place. You can open an SSH shell session on demand, deploy applications, and see the health and status for every device. All of this functionality works no matter where your edge devices are physically located, even when they are behind NAT layers and firewalls. Use the Darcy Cloud to make building, deploying, and debugging easier, and then use it to operate your edge AI applications in production systems.","title":"About the Darcy Cloud"},{"location":"deploy/#add-your-devices-to-the-darcy-cloud","text":"If you don't already have an account, you can create one now for free. Create an account or log in at https://cloud.darcy.ai . Once you are in your Darcy Cloud account, add your device as a node in your current project. Use the \"plus button\" in the bottom left to add a node. Follow the instructions in the pop-up window to add your device as a node.","title":"Add your devices to the Darcy Cloud"},{"location":"deploy/#create-your-application-yaml","text":"Here is a sample YAML file to work with. kind: Application apiVersion: iofog.org/v3 metadata: name: your-application-name spec: microservices: - name: your-microservice-name agent: name: your-darcy-cloud-node-name images: arm: 'YOUR_ORGANIZATION/YOUR_APP:tag.goes.here' x86: 'YOUR_ORGANIZATION/YOUR_APP:tag.goes.here' container: rootHostAccess: true ports: [] volumes: - containerDestination: /dev hostDestination: /dev type: bind accessMode: rw You can find this sample YAMl file in the examples/deploy/ directory called app_deployment.yml . Your application deployment YAML file contains the information that the Darcy Cloud uses to load and run your Darcy AI application on any device. Replace the placeholder fields with your own information and save the file with whatever file name you like, such as my-app-deploy.yml . For the agent name, which is shown above as your-darcy-cloud-node-name you should use the actual node name from your Darcy Cloud account. This is the name that shows for your device which you added in the steps above.","title":"Create your application YAML"},{"location":"deploy/#deploy-your-darcy-ai-application","text":"Now that you have all of the pieces, it's easy to deploy your application to your device or any other device. In the Darcy Cloud, click on the \"plus button\" in the bottom left and choose \"app\". In the pop-up window, choose the \"upload your app\" option and you will see a drag-and-drop window on the right-hand side. You can drag and drop your YAML file into that window or you can click the \"browse and upload\" option and then select your YAML file. The Darcy Cloud will tell you if you have any issues with your YAML file or your app deployment. It will also tell you if your Darcy AI application was deployed successfully. You can then check the status of your application using the Darcy Cloud.","title":"Deploy your Darcy AI application"},{"location":"deploy/#use-your-darcy-ai-application","text":"When your Darcy AI application has successfully been deployed to your devices, you will see the status running in your Darcy Cloud UI. At this time, your Darcy AI application is fully running on those devices. If your application has a live video feed, such as the demo application you built in the Build Guide at port 3456 then you should be able to view the live feed using the IP address of the device followed by :3456 . Replace the IP address in the example below with your device's IP address to view the live feed. http://192.168.1.20:3456 You have accomplished a great amount at this point. Congratulations! You have developed a Darcy AI application and tested it with your IDE and local development environment. You have packaged your application for a variety of target devices. And you have made a deployment YAML file and used the Darcy Cloud to deploy and manage your Darcy AI appliation. Wow!","title":"Use your Darcy AI application"},{"location":"deploy/#next-steps","text":"Now that you have all of these foundation Darcy AI developer skills, you are ready to build full solutions. Use the Technical Documentation to learn more about what Darcy AI can do and take your skills to the next level.","title":"Next steps"},{"location":"getting-started/","text":"Getting started with Darcy AI The Darcy AI SDK offers a rich set of features that Darcy AI application developers can use to easily build complex AI processing chains. This makes developing rich real-time applications possible in a shorter timeframe and with a much more standardized approach. If you have not yet read the overview of Darcy AI terminology, it is recommended that you do that first. You can find it here Darcy AI Terminology Guide . Thinking in terms of Darcy AI pipelines The concept of an AI Pipeline is similar to complex event processing (CEP) and data stream processing, but there are some unique aspects you will notice when building with the Darcy AI. You are allowed only one Darcy AI pipeline in your application. There is a reason for this. A pipeline allows you to order the AI operations in a way that produces predictable trade-offs. One example is placing two different AI operations in parallel. On a hardware system that does not have enough AI accelerator hardware, Darcy will need to make a decision about how to run those operations. If you had more than one pipeline, Darcy would have conflicting intelligence about how to sequence the operations and you would be unable to predict which pipeline would be able to process. You should consider a pipeline to be the backbone of your application flow. You only want one backbone and it should include all of the AI processing that you need in order to make your application run smoothly. The way you structure the pipeline will have an effect on AI processing speed and timing reliability. For processes that must occur in a straight line, attach processing steps called Perceptors one after the other. For processes that can take place in any order and do not depend on one another, you can use parallel ordering. A Darcy AI pipeline is a data graph and can be modeled visually like a sequence tree. p1 (0) p2 (1) p3 (1) / \\ | | p11 (0) p12 (1) p21 (0) p31 (1) | p121 (1) Full trips through the pipeline are your application \"main loop\" Every application has a main processing loop that defines the code structure. Everything is built around that main loop and often gets triggered as part of the main loop\u2019s operations. For your Darcy AI application, you should think of each full trip through the pipeline as one iteration of your main loop. This means you should include all of the AI processing steps that you want to happen every time in your pipeline. Processing steps that should only take place on certain conditions may be best implemented as immediate AI processing (see below) so Darcy does not use precious resources for processing that you don\u2019t want. Start with an Input Stream The first stage of every pipeline cycle (also called a frame or pulse ) is the unprocessed data coming from the Input Stream that you have chosen for your application. Choose an Input Stream that provides the sensor data that you want Darcy to process. This may be audio, video, LiDAR, thermal video, or just about anything you can imagine. A good example of an Input Stream is the CameraStream class that comes built-in with the Darcy AI SDK. This Input Stream allows you to specify the device path for a video camera. It will read the video camera feed and bring it into Darcy at the frame rate and resolution you specify. Instantiate the CameraStream object and set some of its parameters like this: from darcyai.input.camera_stream import CameraStream camera = CameraStream(video_device=\"/dev/video0\", fps=20) Attach a Perceptor The main processing of the Darcy AI Pipeline is found in the Perceptors . Adding a Perceptor is easy. You just instantiate the Perceptor class, perform any initial operations to set it up, and then add it to the Pipeline in whatever position you desire. Each Perceptor offers different configuration options and produces different results. Perceptors also offer events to which you can subscribe. A good example of a powerful Perceptor is the People Perceptor that is built-in with the Darcy AI SDK. This Perceptor is focused on detecting and processing people so you, as the developer, can simply work with semantic data results. Here is an example of creating a People Perceptor instance and adding it to the Pipeline: from darcyai.perceptor.people_perceptor import PeoplePerceptor people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_input_callback) Every pipeline step stores data in the Perception Object Model (POM) When a Perceptor has executed, its results are added to the Perception Object Model (POM) and the Pipeline continues to the next Perceptor or Output Stream if there are not further Perceptors in the Pipeline. The POM is like a shopping cart that gets loaded with data as it moves along. Everything is categorized in the POM so you can easily access the data associated with any Perceptor. The reference is the name you gave that Perceptor when adding it to the Pipeline. Every Perceptor produces its own specific data structure and may also provide convenience functions for performing operations on the result data, such as retrieving a particular person from a set or grabbing the face image of a specific person. The data structure and set of convenience functions for each Perceptor can be found in the documentation for that Perceptor. Here is an example of a callback that is taking advantage of several powerful convenience functions in the POM under the results of the People Perceptor named as \u201cmypeople\u201d: def my_callback(pom, input_data): current_display_frame = pom.mypeople.annotatedFrame().copy() all_people = pom.mypeople.people() current_number_of_people = pom.mypeople.peopleCount() Using Output Streams The last stage of every pipeline cycle is the set of Output Streams that you have added to the pipeline. Any number of Output Streams can be added, giving you the power to put data in many places or perform complex operations such as storing it locally, sending it upstream to a cloud environment, and also displaying a UI at the same time. Output Streams provide a callback which allows you, as the developer, to prepare the data that will be processed by the Output Stream. This is very useful if you want to format data before sending upstream, filter data before storing it on disk, or edit a video frame before you display it. A good example of an Output Stream is the LiveFeedStream class that comes built-in with the Darcy AI SDK. This Output Stream allows you to configure network host and port information and it will open a video feed that you can view with any web browser. Instantiate the LiveFeed output stream object and set some of its parameters like this: from darcyai.output.live_feed_stream import LiveFeedStream def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.mypeople.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} people\".format(pom.mypeople.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) The Perceptor input and output callbacks Every Perceptor that is added to the Pipeline provides both an input callback and output callback. The input callback is available for you, as the developer, to prepare the data that will be passed to the perceptor. The input callback signature is any function that accepts three parameters. The parameters are an input data object, POM object, and configuration object. Pass your input callback function as a parameter when you add the Perceptor to the Pipeline. def people_perceptor_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_perceptor_input_callback) The output callback is intended for you to perform filtering, refinement, editing, and any other data operations on the output of the Perceptor before Darcy moves further down the Pipeline. In many cases, you will not have a need to use this callback. The output of the Perceptor will be usable in its original form. One example of a good use of the output callback, though, is to remove data from the POM that does not fit certain business criteria, such as deleting people from the POM who are facing the wrong direction. Usage of the output callback is similar to the input callback. def people_perceptor_output_callback(perceptor_input, pom, config): #Just take the last person from the results all_people = pom.mypeople.people() filtered_data = None for person in all_people: filtered_data = person return filtered_data people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, output_callback=people_perceptor_output_callback) Configuring Perceptors and Output Streams There are two ways to configure the Perceptors and Output Streams in the Darcy AI system. One approach is to set the configuration in code. The other approach is to use the configuration REST API that becomes available when your Darcy AI application is running. Both approaches also provide the ability for you to fetch the current configuration of both Perceptors and Output Streams. To retrieve the current configuration of a Perceptor or Output Stream in code, call the correct method on the Pipeline object. perceptor_config_dictionary = pipeline.get_perceptor_config(\"mypeople\") outstream_config_dictionary = pipeline.get_output_stream_config(\"videoout\") To set a configuration item in code, call the pipeline method and pass the name of the configuration item as a string and also pass the new value. The list of configuration items will be provided in the Perceptor or Output Stream documentation, along with accepted types of values. pipeline.set_perceptor_config(\"mypeople\", \"show_pose_landmark_dots\", True) To retrieve the current configuration using the REST API, use the following URIs and fill in your device hostname or IP address and replace the Perceptor or Output Stream name with the name you have chosen when adding it to the pipeline. GET http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config GET http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config And to use the REST API to make changes, pass your updated configuration JSON to the following URIs as a PATCH request. PATCH http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config PATCH http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config Subscribing to Perceptor events Perceptors may provide events to which you can subscribe with a callback function. The list of events that a Perceptor provides will be listed in the documentation for that specific Perceptor. To subscribe to an event you pass a callback function with the proper number of parameters as listed in the documentation for that event. Use the .on() method of a Perceptor to subscribe and pass the event name as a string. def new_person_callback(person_id): print(person_id) people_ai = PeoplePerceptor() people_ai.on(\"new_person_entered_scene\", new_person_callback) Immediate and conditional AI processing While the Darcy AI Pipeline is intended for executing AI processing against all incoming data, there are times when you may want to run an AI process against arbitrary data immediately. You may also want to selectively run an AI process under certain conditions but not others. The Pipeline object provides a method for executing a Perceptor against arbitrary data. You can use any Perceptor. The data you pass must be in the form of a StreamData object, which means that you can put any data in the object but you must also add an integer timestamp. people_ai = PeoplePerceptor() current_time = int(time.time()) my_data = StreamData(saved_video_frame, current_time) pipeline.run_perceptor(people_ai, my_data) Bring it all together with a full application The best way to learn all of these concepts is to see them in action. Start with building a sample application that illustrates all of these topics in actual code. Then build your own! You can find a sample application guide here Build Guide .","title":"Getting Started"},{"location":"getting-started/#getting-started-with-darcy-ai","text":"The Darcy AI SDK offers a rich set of features that Darcy AI application developers can use to easily build complex AI processing chains. This makes developing rich real-time applications possible in a shorter timeframe and with a much more standardized approach. If you have not yet read the overview of Darcy AI terminology, it is recommended that you do that first. You can find it here Darcy AI Terminology Guide .","title":"Getting started with Darcy AI"},{"location":"getting-started/#thinking-in-terms-of-darcy-ai-pipelines","text":"The concept of an AI Pipeline is similar to complex event processing (CEP) and data stream processing, but there are some unique aspects you will notice when building with the Darcy AI. You are allowed only one Darcy AI pipeline in your application. There is a reason for this. A pipeline allows you to order the AI operations in a way that produces predictable trade-offs. One example is placing two different AI operations in parallel. On a hardware system that does not have enough AI accelerator hardware, Darcy will need to make a decision about how to run those operations. If you had more than one pipeline, Darcy would have conflicting intelligence about how to sequence the operations and you would be unable to predict which pipeline would be able to process. You should consider a pipeline to be the backbone of your application flow. You only want one backbone and it should include all of the AI processing that you need in order to make your application run smoothly. The way you structure the pipeline will have an effect on AI processing speed and timing reliability. For processes that must occur in a straight line, attach processing steps called Perceptors one after the other. For processes that can take place in any order and do not depend on one another, you can use parallel ordering. A Darcy AI pipeline is a data graph and can be modeled visually like a sequence tree. p1 (0) p2 (1) p3 (1) / \\ | | p11 (0) p12 (1) p21 (0) p31 (1) | p121 (1)","title":"Thinking in terms of Darcy AI pipelines"},{"location":"getting-started/#full-trips-through-the-pipeline-are-your-application-main-loop","text":"Every application has a main processing loop that defines the code structure. Everything is built around that main loop and often gets triggered as part of the main loop\u2019s operations. For your Darcy AI application, you should think of each full trip through the pipeline as one iteration of your main loop. This means you should include all of the AI processing steps that you want to happen every time in your pipeline. Processing steps that should only take place on certain conditions may be best implemented as immediate AI processing (see below) so Darcy does not use precious resources for processing that you don\u2019t want.","title":"Full trips through the pipeline are your application \"main loop\""},{"location":"getting-started/#start-with-an-input-stream","text":"The first stage of every pipeline cycle (also called a frame or pulse ) is the unprocessed data coming from the Input Stream that you have chosen for your application. Choose an Input Stream that provides the sensor data that you want Darcy to process. This may be audio, video, LiDAR, thermal video, or just about anything you can imagine. A good example of an Input Stream is the CameraStream class that comes built-in with the Darcy AI SDK. This Input Stream allows you to specify the device path for a video camera. It will read the video camera feed and bring it into Darcy at the frame rate and resolution you specify. Instantiate the CameraStream object and set some of its parameters like this: from darcyai.input.camera_stream import CameraStream camera = CameraStream(video_device=\"/dev/video0\", fps=20)","title":"Start with an Input Stream"},{"location":"getting-started/#attach-a-perceptor","text":"The main processing of the Darcy AI Pipeline is found in the Perceptors . Adding a Perceptor is easy. You just instantiate the Perceptor class, perform any initial operations to set it up, and then add it to the Pipeline in whatever position you desire. Each Perceptor offers different configuration options and produces different results. Perceptors also offer events to which you can subscribe. A good example of a powerful Perceptor is the People Perceptor that is built-in with the Darcy AI SDK. This Perceptor is focused on detecting and processing people so you, as the developer, can simply work with semantic data results. Here is an example of creating a People Perceptor instance and adding it to the Pipeline: from darcyai.perceptor.people_perceptor import PeoplePerceptor people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_input_callback)","title":"Attach a Perceptor"},{"location":"getting-started/#every-pipeline-step-stores-data-in-the-perception-object-model-pom","text":"When a Perceptor has executed, its results are added to the Perception Object Model (POM) and the Pipeline continues to the next Perceptor or Output Stream if there are not further Perceptors in the Pipeline. The POM is like a shopping cart that gets loaded with data as it moves along. Everything is categorized in the POM so you can easily access the data associated with any Perceptor. The reference is the name you gave that Perceptor when adding it to the Pipeline. Every Perceptor produces its own specific data structure and may also provide convenience functions for performing operations on the result data, such as retrieving a particular person from a set or grabbing the face image of a specific person. The data structure and set of convenience functions for each Perceptor can be found in the documentation for that Perceptor. Here is an example of a callback that is taking advantage of several powerful convenience functions in the POM under the results of the People Perceptor named as \u201cmypeople\u201d: def my_callback(pom, input_data): current_display_frame = pom.mypeople.annotatedFrame().copy() all_people = pom.mypeople.people() current_number_of_people = pom.mypeople.peopleCount()","title":"Every pipeline step stores data in the Perception Object Model (POM)"},{"location":"getting-started/#using-output-streams","text":"The last stage of every pipeline cycle is the set of Output Streams that you have added to the pipeline. Any number of Output Streams can be added, giving you the power to put data in many places or perform complex operations such as storing it locally, sending it upstream to a cloud environment, and also displaying a UI at the same time. Output Streams provide a callback which allows you, as the developer, to prepare the data that will be processed by the Output Stream. This is very useful if you want to format data before sending upstream, filter data before storing it on disk, or edit a video frame before you display it. A good example of an Output Stream is the LiveFeedStream class that comes built-in with the Darcy AI SDK. This Output Stream allows you to configure network host and port information and it will open a video feed that you can view with any web browser. Instantiate the LiveFeed output stream object and set some of its parameters like this: from darcyai.output.live_feed_stream import LiveFeedStream def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.mypeople.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} people\".format(pom.mypeople.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") pipeline.add_output_stream(\"output\", live_feed_callback, live_feed)","title":"Using Output Streams"},{"location":"getting-started/#the-perceptor-input-and-output-callbacks","text":"Every Perceptor that is added to the Pipeline provides both an input callback and output callback. The input callback is available for you, as the developer, to prepare the data that will be passed to the perceptor. The input callback signature is any function that accepts three parameters. The parameters are an input data object, POM object, and configuration object. Pass your input callback function as a parameter when you add the Perceptor to the Pipeline. def people_perceptor_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_perceptor_input_callback) The output callback is intended for you to perform filtering, refinement, editing, and any other data operations on the output of the Perceptor before Darcy moves further down the Pipeline. In many cases, you will not have a need to use this callback. The output of the Perceptor will be usable in its original form. One example of a good use of the output callback, though, is to remove data from the POM that does not fit certain business criteria, such as deleting people from the POM who are facing the wrong direction. Usage of the output callback is similar to the input callback. def people_perceptor_output_callback(perceptor_input, pom, config): #Just take the last person from the results all_people = pom.mypeople.people() filtered_data = None for person in all_people: filtered_data = person return filtered_data people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, output_callback=people_perceptor_output_callback)","title":"The Perceptor input and output callbacks"},{"location":"getting-started/#configuring-perceptors-and-output-streams","text":"There are two ways to configure the Perceptors and Output Streams in the Darcy AI system. One approach is to set the configuration in code. The other approach is to use the configuration REST API that becomes available when your Darcy AI application is running. Both approaches also provide the ability for you to fetch the current configuration of both Perceptors and Output Streams. To retrieve the current configuration of a Perceptor or Output Stream in code, call the correct method on the Pipeline object. perceptor_config_dictionary = pipeline.get_perceptor_config(\"mypeople\") outstream_config_dictionary = pipeline.get_output_stream_config(\"videoout\") To set a configuration item in code, call the pipeline method and pass the name of the configuration item as a string and also pass the new value. The list of configuration items will be provided in the Perceptor or Output Stream documentation, along with accepted types of values. pipeline.set_perceptor_config(\"mypeople\", \"show_pose_landmark_dots\", True) To retrieve the current configuration using the REST API, use the following URIs and fill in your device hostname or IP address and replace the Perceptor or Output Stream name with the name you have chosen when adding it to the pipeline. GET http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config GET http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config And to use the REST API to make changes, pass your updated configuration JSON to the following URIs as a PATCH request. PATCH http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config PATCH http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config","title":"Configuring Perceptors and Output Streams"},{"location":"getting-started/#subscribing-to-perceptor-events","text":"Perceptors may provide events to which you can subscribe with a callback function. The list of events that a Perceptor provides will be listed in the documentation for that specific Perceptor. To subscribe to an event you pass a callback function with the proper number of parameters as listed in the documentation for that event. Use the .on() method of a Perceptor to subscribe and pass the event name as a string. def new_person_callback(person_id): print(person_id) people_ai = PeoplePerceptor() people_ai.on(\"new_person_entered_scene\", new_person_callback)","title":"Subscribing to Perceptor events"},{"location":"getting-started/#immediate-and-conditional-ai-processing","text":"While the Darcy AI Pipeline is intended for executing AI processing against all incoming data, there are times when you may want to run an AI process against arbitrary data immediately. You may also want to selectively run an AI process under certain conditions but not others. The Pipeline object provides a method for executing a Perceptor against arbitrary data. You can use any Perceptor. The data you pass must be in the form of a StreamData object, which means that you can put any data in the object but you must also add an integer timestamp. people_ai = PeoplePerceptor() current_time = int(time.time()) my_data = StreamData(saved_video_frame, current_time) pipeline.run_perceptor(people_ai, my_data)","title":"Immediate and conditional AI processing"},{"location":"getting-started/#bring-it-all-together-with-a-full-application","text":"The best way to learn all of these concepts is to see them in action. Start with building a sample application that illustrates all of these topics in actual code. Then build your own! You can find a sample application guide here Build Guide .","title":"Bring it all together with a full application"},{"location":"jetson-nano-setup/","text":"Setting up your Jetson Nano for Darcy AI development Nvidia Jetson Nano boards are excellent for building and running Darcy AI applications. This guide will show you how to get your Nano ready for Darcy AI development so you can run and debug your applications. Note that you will be using a Google Coral AI accelerator attached to your Jetson Nano. Although the Jetson Nano board contains an Nvidia GPU, the Darcy AI platform currently requires a Google Coral accelerator in order to operate. New editions of Darcy AI that take advantage of the Nvidia GPU will be available soon. Hardware you will need Nvidia Jetson Nano board (Nano developer kit version with 4GB of RAM recommended) Video camera attached to the camera port (any camera compatible with Raspberry Pi or any USB camera will work) Google Coral edge TPU (USB version attached to USB 3.0 port) Micro SD card with 32GB+ capacity and UHS-1 speed rating or faster Power supply with 5.5mm barrel plug at 5 Volts DC with at least 3 Amps output Jumper for power pins on Nano board (alternative hardware suggested below) Follow the Nvidia Jetston Nano guide to setup your board You will need to flash your SD card with the operating system and developer tools provided by Nvidia. Then your Jetson Nano will boot to the operating system and allow you to SSH into the board. Nvidia provides an excellent guide for getting started. It includes instructions for users on Windows, Mac OS, and Linux computers. If you are using the \"headless\" setup approach, you will need to put a jumper on your Nano board on pin header J48. You can use a standard 2-pin motherboard jumper but you can also use any approach that will make an electrical connection between these two pins. One approach is to use one end of an aligator clip by attaching the jaws across both pins. Follow the official Jetson Nano guide here: https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit Open a command line session The goal of the prior step is to get your Jetson Nano set up with JetPack and an operating system so it will boot. If you accomplished the prior step successfully, you should be able to use SSH, terminal session over USB, or an attached keyboard, monitor, and mouse to open a command line session. No matter which approach you used to setup your Jetson Nano (headless or attached monitor), you should now be able to log in and see a command line prompt. Linux package manager update Next you will be installing a set of software packages. Some of them are Python packages and some are Linux packages. Start by updating the Ubuntu package manager with the following command. It may take a minute or two. sudo apt-get update Install curl Curl is a very useful package for downloading files from the command line. You will use it later when installing the Google Coral software. Install it now with the following command. sudo apt-get install curl Install JPEG codec In order for some Python packages to work, your Jetson Nano needs to have a JPEG image codec available. A JPEG library is not installed by default with the JetPack software. Install one with this command. sudo apt-get install libjpeg-dev Install Pip3 package manager for Python3 By default, the JetPack software installs Python 3.6.9 and also a version of Python 2.7. The package manager for Python, called Pip, is only installed for the Python 2.7 version. You need to install Pip3 which is a Python3 package manager. Use the following command to install Pip3. sudo apt-get install python3-pip Edit .bashrc file to use Python3 and Pip3 Now that Pip3 has been installed, you need to edit a file that will make Python3 your default so you can just use the word python in your commands instead of python3 . You will also be able to just use pip for installing Pip3 packages after you make this edit. You can use the vi editor as in the example here or you can use any other text editor you prefer. Use this command to begin editing your .bashrc file. vi ~/.bashrc Navigate to the bottom of the file. You will add two lines to the very end of the file. Add the following lines. alias python=python3 alias pip=pip3 Save the file and exit the editor. If you are using the vi editor you can do this by pressing the esc key and then typing :wq and pressing enter . Now that you have edited the file, it will take effect every time you log in. And now when you use python in a command, it will use Python3 and every time you use pip in a command it will use Pip3. Activate the edited .bashrc file now so you don't have to wait until the next login. Use the following command. source ~/.bashrc Install Pillow Python library Install the imutils package for Python with the command pip install Pillow . Install imutils Python library Install the imutils package for Python with the command pip install imutils . Install DarcyAI Python library Install the DarcyAI pacakge for Python with the command pip install darcyai . Add your user to the Docker Linux group Docker is installed by default on your Jetson Nano when you use the JetPack software package from Nvidia. You need to add your Ubuntu Linux user to the docker group so you can execute Docker commands without using sudo . Your username is the login name you chose during the Jetson Nano setup process at the start of this guide. Replace the word YOURUSER shown below with your actual user such as darcydev sudo usermod -aG docker YOURUSER Install Google Coral software libraries You need the PyCoral and Edge TPU runtime software libraries installed on your Jetson Nano. Follow the guide provided by Google and use the sections marked as \"on Linux\". https://coral.ai/docs/accelerator/get-started/#requirements Reboot your Jetson Nano You will need to reboot your Jetson Nano after finishing all of these installation steps. Make sure your camera and your Google Coral USB device are attached to your Nano board when you reboot. sudo reboot Check for Google Coral USB device(s) You can see which devices are attached to your Jetson Nano using the command lsusb . This will display a list of devices. Look for an item that shows Global Unichip Corp. . You will have more than one of these items if you have more than one Google Coral USB device attached. You will notice that once you start running a Darcy AI application, the value shown by lsusb will change to Google, Inc. . This is normal and expected and whenever you reboot your Nano board it will return to Global Unichip Corp. temporarily. Run the system check script Run the script called check.bash to scan your Jetson Nano and make sure everything is looking good. If you receive an error, use the error message to pinpoint which step needs to be completed or fixed. If you receive a message that everything is looking good, then you are ready to build with Darcy AI! Start building your AI applications with Darcy Go to the Build Guide and get started building with Darcy AI!","title":"Jetson Nano Setup"},{"location":"jetson-nano-setup/#setting-up-your-jetson-nano-for-darcy-ai-development","text":"Nvidia Jetson Nano boards are excellent for building and running Darcy AI applications. This guide will show you how to get your Nano ready for Darcy AI development so you can run and debug your applications. Note that you will be using a Google Coral AI accelerator attached to your Jetson Nano. Although the Jetson Nano board contains an Nvidia GPU, the Darcy AI platform currently requires a Google Coral accelerator in order to operate. New editions of Darcy AI that take advantage of the Nvidia GPU will be available soon.","title":"Setting up your Jetson Nano for Darcy AI development"},{"location":"jetson-nano-setup/#hardware-you-will-need","text":"Nvidia Jetson Nano board (Nano developer kit version with 4GB of RAM recommended) Video camera attached to the camera port (any camera compatible with Raspberry Pi or any USB camera will work) Google Coral edge TPU (USB version attached to USB 3.0 port) Micro SD card with 32GB+ capacity and UHS-1 speed rating or faster Power supply with 5.5mm barrel plug at 5 Volts DC with at least 3 Amps output Jumper for power pins on Nano board (alternative hardware suggested below)","title":"Hardware you will need"},{"location":"jetson-nano-setup/#follow-the-nvidia-jetston-nano-guide-to-setup-your-board","text":"You will need to flash your SD card with the operating system and developer tools provided by Nvidia. Then your Jetson Nano will boot to the operating system and allow you to SSH into the board. Nvidia provides an excellent guide for getting started. It includes instructions for users on Windows, Mac OS, and Linux computers. If you are using the \"headless\" setup approach, you will need to put a jumper on your Nano board on pin header J48. You can use a standard 2-pin motherboard jumper but you can also use any approach that will make an electrical connection between these two pins. One approach is to use one end of an aligator clip by attaching the jaws across both pins. Follow the official Jetson Nano guide here: https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit","title":"Follow the Nvidia Jetston Nano guide to setup your board"},{"location":"jetson-nano-setup/#open-a-command-line-session","text":"The goal of the prior step is to get your Jetson Nano set up with JetPack and an operating system so it will boot. If you accomplished the prior step successfully, you should be able to use SSH, terminal session over USB, or an attached keyboard, monitor, and mouse to open a command line session. No matter which approach you used to setup your Jetson Nano (headless or attached monitor), you should now be able to log in and see a command line prompt.","title":"Open a command line session"},{"location":"jetson-nano-setup/#linux-package-manager-update","text":"Next you will be installing a set of software packages. Some of them are Python packages and some are Linux packages. Start by updating the Ubuntu package manager with the following command. It may take a minute or two. sudo apt-get update","title":"Linux package manager update"},{"location":"jetson-nano-setup/#install-curl","text":"Curl is a very useful package for downloading files from the command line. You will use it later when installing the Google Coral software. Install it now with the following command. sudo apt-get install curl","title":"Install curl"},{"location":"jetson-nano-setup/#install-jpeg-codec","text":"In order for some Python packages to work, your Jetson Nano needs to have a JPEG image codec available. A JPEG library is not installed by default with the JetPack software. Install one with this command. sudo apt-get install libjpeg-dev","title":"Install JPEG codec"},{"location":"jetson-nano-setup/#install-pip3-package-manager-for-python3","text":"By default, the JetPack software installs Python 3.6.9 and also a version of Python 2.7. The package manager for Python, called Pip, is only installed for the Python 2.7 version. You need to install Pip3 which is a Python3 package manager. Use the following command to install Pip3. sudo apt-get install python3-pip","title":"Install Pip3 package manager for Python3"},{"location":"jetson-nano-setup/#edit-bashrc-file-to-use-python3-and-pip3","text":"Now that Pip3 has been installed, you need to edit a file that will make Python3 your default so you can just use the word python in your commands instead of python3 . You will also be able to just use pip for installing Pip3 packages after you make this edit. You can use the vi editor as in the example here or you can use any other text editor you prefer. Use this command to begin editing your .bashrc file. vi ~/.bashrc Navigate to the bottom of the file. You will add two lines to the very end of the file. Add the following lines. alias python=python3 alias pip=pip3 Save the file and exit the editor. If you are using the vi editor you can do this by pressing the esc key and then typing :wq and pressing enter . Now that you have edited the file, it will take effect every time you log in. And now when you use python in a command, it will use Python3 and every time you use pip in a command it will use Pip3. Activate the edited .bashrc file now so you don't have to wait until the next login. Use the following command. source ~/.bashrc","title":"Edit .bashrc file to use Python3 and Pip3"},{"location":"jetson-nano-setup/#install-pillow-python-library","text":"Install the imutils package for Python with the command pip install Pillow .","title":"Install Pillow Python library"},{"location":"jetson-nano-setup/#install-imutils-python-library","text":"Install the imutils package for Python with the command pip install imutils .","title":"Install imutils Python library"},{"location":"jetson-nano-setup/#install-darcyai-python-library","text":"Install the DarcyAI pacakge for Python with the command pip install darcyai .","title":"Install DarcyAI Python library"},{"location":"jetson-nano-setup/#add-your-user-to-the-docker-linux-group","text":"Docker is installed by default on your Jetson Nano when you use the JetPack software package from Nvidia. You need to add your Ubuntu Linux user to the docker group so you can execute Docker commands without using sudo . Your username is the login name you chose during the Jetson Nano setup process at the start of this guide. Replace the word YOURUSER shown below with your actual user such as darcydev sudo usermod -aG docker YOURUSER","title":"Add your user to the Docker Linux group"},{"location":"jetson-nano-setup/#install-google-coral-software-libraries","text":"You need the PyCoral and Edge TPU runtime software libraries installed on your Jetson Nano. Follow the guide provided by Google and use the sections marked as \"on Linux\". https://coral.ai/docs/accelerator/get-started/#requirements","title":"Install Google Coral software libraries"},{"location":"jetson-nano-setup/#reboot-your-jetson-nano","text":"You will need to reboot your Jetson Nano after finishing all of these installation steps. Make sure your camera and your Google Coral USB device are attached to your Nano board when you reboot. sudo reboot","title":"Reboot your Jetson Nano"},{"location":"jetson-nano-setup/#check-for-google-coral-usb-devices","text":"You can see which devices are attached to your Jetson Nano using the command lsusb . This will display a list of devices. Look for an item that shows Global Unichip Corp. . You will have more than one of these items if you have more than one Google Coral USB device attached. You will notice that once you start running a Darcy AI application, the value shown by lsusb will change to Google, Inc. . This is normal and expected and whenever you reboot your Nano board it will return to Global Unichip Corp. temporarily.","title":"Check for Google Coral USB device(s)"},{"location":"jetson-nano-setup/#run-the-system-check-script","text":"Run the script called check.bash to scan your Jetson Nano and make sure everything is looking good. If you receive an error, use the error message to pinpoint which step needs to be completed or fixed. If you receive a message that everything is looking good, then you are ready to build with Darcy AI!","title":"Run the system check script"},{"location":"jetson-nano-setup/#start-building-your-ai-applications-with-darcy","text":"Go to the Build Guide and get started building with Darcy AI!","title":"Start building your AI applications with Darcy"},{"location":"macos-setup/","text":"Setting up your Mac OS X environment for Darcy AI development You can use any Mac OS X environment to build Darcy AI applications. After you finish setting up your Mac as a development environment, you will be able to write, test, and debug your Darcy AI apps. Then you can package your code and deploy the applications to any edge device! What you will need Mac OS X laptop or desktop (both x86 and M1 processor Macs will work) A video camera that you want to use (the built-in webcam works very well for this purpose) 5GB or more of free disk space to accommodate code libraries and application container images Any IDE software that will allow you to write and debug Python (use your favorite IDE) Install Python 3.6.9 or greater If you do not already have Python version 3.6.9 or greater, you will need to install it now. Darcy AI requires this version of Python or higher. Note that Python 2.x versions are also not compatible with Darcy AI. You need Python 3 and the darcyai library will not install with versions below 3.6.9. Download and install the latest or your preferred version of Python3 from Python.org directly https://www.python.org/downloads/macos/ . If you have both Python 2.x versions and Python 3.x versions on your Mac, you may need to use python3 and pip3 for all of the commands instead of just python and pip . You can change this if you want by following this guide https://osxdaily.com/2022/02/15/make-python-3-default-macos/ . Install OpenCV Install the OpenCV package for Python with the command pip install opencv-python Install the Pillow library Install the Pillow package for Python with the command pip install Pillow . Install the Numpy library Install the numpy package for Python with the command pip install numpy . Install the Imutils library Install the imutils package for Python with the command pip install imutils . Install the DarcyAI library Install the Darcy AI library for Python with the command pip install darcyai . Install Docker for Mac If you don't already have Docker on your Mac, install it now by following the official instructions https://docs.docker.com/desktop/mac/install/ . After you have installed Docker, you can use docker commands in terminal. You will be using these commands to package your Darcy AI applications for deployment, including deploying to edge devices that are a different CPU architecture than your Mac! To make sure you can use the latest Docker build commands like buildx you can add an environment variable to your Mac with the following command export DOCKER_CLI_EXPERIMENTAL=enabled . This will tell Docker to allow use of the latest tools which will save you a lot of time when packaging your apps! Note that you may need to use sudo docker instead of just docker depending on how you install and set up Docker for Mac. If that is the case on your development machine, you can just add sudo to the beginning of any docker commands shown in these guides. Install TensorFlow Install TensorFlow for Python with the command pip install tensorflow Start building your AI applications with Darcy Go to the Build Guide and get started building with Darcy AI!","title":"macOS Setup"},{"location":"macos-setup/#setting-up-your-mac-os-x-environment-for-darcy-ai-development","text":"You can use any Mac OS X environment to build Darcy AI applications. After you finish setting up your Mac as a development environment, you will be able to write, test, and debug your Darcy AI apps. Then you can package your code and deploy the applications to any edge device!","title":"Setting up your Mac OS X environment for Darcy AI development"},{"location":"macos-setup/#what-you-will-need","text":"Mac OS X laptop or desktop (both x86 and M1 processor Macs will work) A video camera that you want to use (the built-in webcam works very well for this purpose) 5GB or more of free disk space to accommodate code libraries and application container images Any IDE software that will allow you to write and debug Python (use your favorite IDE)","title":"What you will need"},{"location":"macos-setup/#install-python-369-or-greater","text":"If you do not already have Python version 3.6.9 or greater, you will need to install it now. Darcy AI requires this version of Python or higher. Note that Python 2.x versions are also not compatible with Darcy AI. You need Python 3 and the darcyai library will not install with versions below 3.6.9. Download and install the latest or your preferred version of Python3 from Python.org directly https://www.python.org/downloads/macos/ . If you have both Python 2.x versions and Python 3.x versions on your Mac, you may need to use python3 and pip3 for all of the commands instead of just python and pip . You can change this if you want by following this guide https://osxdaily.com/2022/02/15/make-python-3-default-macos/ .","title":"Install Python 3.6.9 or greater"},{"location":"macos-setup/#install-opencv","text":"Install the OpenCV package for Python with the command pip install opencv-python","title":"Install OpenCV"},{"location":"macos-setup/#install-the-pillow-library","text":"Install the Pillow package for Python with the command pip install Pillow .","title":"Install the Pillow library"},{"location":"macos-setup/#install-the-numpy-library","text":"Install the numpy package for Python with the command pip install numpy .","title":"Install the Numpy library"},{"location":"macos-setup/#install-the-imutils-library","text":"Install the imutils package for Python with the command pip install imutils .","title":"Install the Imutils library"},{"location":"macos-setup/#install-the-darcyai-library","text":"Install the Darcy AI library for Python with the command pip install darcyai .","title":"Install the DarcyAI library"},{"location":"macos-setup/#install-docker-for-mac","text":"If you don't already have Docker on your Mac, install it now by following the official instructions https://docs.docker.com/desktop/mac/install/ . After you have installed Docker, you can use docker commands in terminal. You will be using these commands to package your Darcy AI applications for deployment, including deploying to edge devices that are a different CPU architecture than your Mac! To make sure you can use the latest Docker build commands like buildx you can add an environment variable to your Mac with the following command export DOCKER_CLI_EXPERIMENTAL=enabled . This will tell Docker to allow use of the latest tools which will save you a lot of time when packaging your apps! Note that you may need to use sudo docker instead of just docker depending on how you install and set up Docker for Mac. If that is the case on your development machine, you can just add sudo to the beginning of any docker commands shown in these guides.","title":"Install Docker for Mac"},{"location":"macos-setup/#install-tensorflow","text":"Install TensorFlow for Python with the command pip install tensorflow","title":"Install TensorFlow"},{"location":"macos-setup/#start-building-your-ai-applications-with-darcy","text":"Go to the Build Guide and get started building with Darcy AI!","title":"Start building your AI applications with Darcy"},{"location":"package/","text":"Packaging your Darcy AI application Once you have a Darcy AI application built and tested, the next step is to package it for deployment. Packaging pulls together your Python code, the Darcy AI libraries, and anything else needed to run your application and puts it all into a set of Docker containers that will run on a wide variety of hardware devices. What you will accomplish By the end of this guide, you will be able to make deployable Darcy AI application packages that will run on any hardware that meets the requirements for Darcy AI. The list of compatible hardware is extensive, allowing you to deploy your Darcy AI applications to the devices that fit your solution needs. Requirements In order to package your Darcy AI applications, you only need to meet a couple of requirements: - Docker Desktop for Mac, Windows, or Linux - A Docker Hub account for hosting your container images (or another registry compatible with Docker) Overview of the process To perform the packaging process, you will simply need to follow a few steps. - Create a Dockerfile so Docker knows how to build your container images - Open a builder namespace - Build the container images for multiple platforms (different CPU architectures) - Upload your container images to your Docker Hub account or similar container registry Add a Dockerfile to the same directory as your Python file To build your Darcy AI application container, you only need your Python file and a Dockerfile. A Dockerfile is just a text file with the specific name Dockerfile that tells the Docker command tools how to make your containers. You can include as many files as you want in your container. The commands for adding those files are discussed below. Make sure you create the Dockerfile in the same directory as your Python file and change the name below from YOURFILE.py to the actual name of your file. FROM darcyai/darcy-ai-coral:dev RUN python3 -m pip install darcyai COPY ./YOURFILE.py /src/app.py CMD python3 -u /src/app.py The FROM command tells Docker which base image to use. It will build your application container starting from the base image. Every RUN command tells Docker to execute a step. In the example above, the step is to install the darcyai Python library. We don't include that library in the base image because that would make it more difficult to use the latest Darcy AI library. Using this single command in your Dockerfile, you will always get the latest Darcy AI library when building your container images. Similarly, every COPY command tells Docker to take something from your local environment and make a copy of it in your container. Use this command to copy in files that are part of your application, such as .mp4 videos, .tflite AI models, and additional Python code files. The first part of the command is the source and the second part is the destination. In the example above, the YOURFILE.py file is copied into the /src/ directory in the container and renamed to app.py . The CMD command tells Docker to execute this command when the container is started. This is different than the RUN command which tells Docker to execute the command while building the container. The CMD statement is found at the end because the container must be fully built before this statement. When the container starts, the instructions found after the CMD will be executed. In the example above, the instructions are to run the /src/app.py Python file using python3 and we have added the -u parameter which tells the Python3 engine to use unbuffered output because we want to see the output in the container logs unhindered. Create a builder namespace for your build process The docker buildx command line tool that was installed with your Docker Desktop will allow you to build and package container images for several target device platforms (CPU architectures) at the same time. If you do not have the docker buildx tool installed, you can learn about it and install it from the Docker BuildX Guide . The first step is to create a named builder that BuildX can use. You can do that with the following command. Replace YOURNAME with the name you would like to use. NOTE: If your installation of Docker Desktop requires you to use sudo when using docker commands, simply add the sudo to the beginning of everything shown in this guide. docker buildx create --name YOURNAME And now that you have created a builder namespace, let's set BuildX to use that namespace with this command. docker buildx use YOURNAME Build your Docker container Now that you have a working BuildX builder namespace and a Dockerfile in your current working directory where your Python file is located, you can do the actual build. NOTE: If you don't already have an account, create one now at https://hub.docker.com . You will be given an organization which is your username. Use the following command to perform the build. You will need to replace organization with your actual Docker Hub organization name. Also replace application-name with the name you want to use for this container. The part after the : is the tag. You can put anything you want here. It is a common practice to put a version number, such as 1.0.0 in the example below. docker buildx build -t organization/application-name:1.0.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --push . The --platform part of this build command specifies the platforms for which you want containers built. It is recommended to build for the list of platforms shown in the example here. This will allow you to run your Darcy AI application container on 64-bit x86 devices and both 64-bit and 32-bit ARM devices. The --push part of the command tells Docker to upload your container images to Docker Hub when it is finished building. Don't forget the . on the end of the command. That tells the BuildX tool to look for your Dockerfile in the current directory. Your build process may take 10 or 15 minutes if you are building for the first time and you do not have a very fast internet connection. This is because the underlying container base images will need to be downloaded. After the first build, this process should only take a few minutes. You can watch the output of the command to see the build progress. A separate container image will be built for each of the platforms specified in the command. Additionally a container manifest file will be created and added to the container registry (Docker Hub) so different platforms will know which image to download and start. Next step is to deploy your Darcy AI application Now you have a fully packaged Darcy AI application! The next step is to learn how to deploy. Follow the Deployment Guide to learn how to deploy your packaged Darcy AI apps.","title":"Packaging"},{"location":"package/#packaging-your-darcy-ai-application","text":"Once you have a Darcy AI application built and tested, the next step is to package it for deployment. Packaging pulls together your Python code, the Darcy AI libraries, and anything else needed to run your application and puts it all into a set of Docker containers that will run on a wide variety of hardware devices.","title":"Packaging your Darcy AI application"},{"location":"package/#what-you-will-accomplish","text":"By the end of this guide, you will be able to make deployable Darcy AI application packages that will run on any hardware that meets the requirements for Darcy AI. The list of compatible hardware is extensive, allowing you to deploy your Darcy AI applications to the devices that fit your solution needs.","title":"What you will accomplish"},{"location":"package/#requirements","text":"In order to package your Darcy AI applications, you only need to meet a couple of requirements: - Docker Desktop for Mac, Windows, or Linux - A Docker Hub account for hosting your container images (or another registry compatible with Docker)","title":"Requirements"},{"location":"package/#overview-of-the-process","text":"To perform the packaging process, you will simply need to follow a few steps. - Create a Dockerfile so Docker knows how to build your container images - Open a builder namespace - Build the container images for multiple platforms (different CPU architectures) - Upload your container images to your Docker Hub account or similar container registry","title":"Overview of the process"},{"location":"package/#add-a-dockerfile-to-the-same-directory-as-your-python-file","text":"To build your Darcy AI application container, you only need your Python file and a Dockerfile. A Dockerfile is just a text file with the specific name Dockerfile that tells the Docker command tools how to make your containers. You can include as many files as you want in your container. The commands for adding those files are discussed below. Make sure you create the Dockerfile in the same directory as your Python file and change the name below from YOURFILE.py to the actual name of your file. FROM darcyai/darcy-ai-coral:dev RUN python3 -m pip install darcyai COPY ./YOURFILE.py /src/app.py CMD python3 -u /src/app.py The FROM command tells Docker which base image to use. It will build your application container starting from the base image. Every RUN command tells Docker to execute a step. In the example above, the step is to install the darcyai Python library. We don't include that library in the base image because that would make it more difficult to use the latest Darcy AI library. Using this single command in your Dockerfile, you will always get the latest Darcy AI library when building your container images. Similarly, every COPY command tells Docker to take something from your local environment and make a copy of it in your container. Use this command to copy in files that are part of your application, such as .mp4 videos, .tflite AI models, and additional Python code files. The first part of the command is the source and the second part is the destination. In the example above, the YOURFILE.py file is copied into the /src/ directory in the container and renamed to app.py . The CMD command tells Docker to execute this command when the container is started. This is different than the RUN command which tells Docker to execute the command while building the container. The CMD statement is found at the end because the container must be fully built before this statement. When the container starts, the instructions found after the CMD will be executed. In the example above, the instructions are to run the /src/app.py Python file using python3 and we have added the -u parameter which tells the Python3 engine to use unbuffered output because we want to see the output in the container logs unhindered.","title":"Add a Dockerfile to the same directory as your Python file"},{"location":"package/#create-a-builder-namespace-for-your-build-process","text":"The docker buildx command line tool that was installed with your Docker Desktop will allow you to build and package container images for several target device platforms (CPU architectures) at the same time. If you do not have the docker buildx tool installed, you can learn about it and install it from the Docker BuildX Guide . The first step is to create a named builder that BuildX can use. You can do that with the following command. Replace YOURNAME with the name you would like to use. NOTE: If your installation of Docker Desktop requires you to use sudo when using docker commands, simply add the sudo to the beginning of everything shown in this guide. docker buildx create --name YOURNAME And now that you have created a builder namespace, let's set BuildX to use that namespace with this command. docker buildx use YOURNAME","title":"Create a builder namespace for your build process"},{"location":"package/#build-your-docker-container","text":"Now that you have a working BuildX builder namespace and a Dockerfile in your current working directory where your Python file is located, you can do the actual build. NOTE: If you don't already have an account, create one now at https://hub.docker.com . You will be given an organization which is your username. Use the following command to perform the build. You will need to replace organization with your actual Docker Hub organization name. Also replace application-name with the name you want to use for this container. The part after the : is the tag. You can put anything you want here. It is a common practice to put a version number, such as 1.0.0 in the example below. docker buildx build -t organization/application-name:1.0.0 --platform linux/amd64,linux/arm64,linux/arm/v7 --push . The --platform part of this build command specifies the platforms for which you want containers built. It is recommended to build for the list of platforms shown in the example here. This will allow you to run your Darcy AI application container on 64-bit x86 devices and both 64-bit and 32-bit ARM devices. The --push part of the command tells Docker to upload your container images to Docker Hub when it is finished building. Don't forget the . on the end of the command. That tells the BuildX tool to look for your Dockerfile in the current directory. Your build process may take 10 or 15 minutes if you are building for the first time and you do not have a very fast internet connection. This is because the underlying container base images will need to be downloaded. After the first build, this process should only take a few minutes. You can watch the output of the command to see the build progress. A separate container image will be built for each of the platforms specified in the command. Additionally a container manifest file will be created and added to the container registry (Docker Hub) so different platforms will know which image to download and start.","title":"Build your Docker container"},{"location":"package/#next-step-is-to-deploy-your-darcy-ai-application","text":"Now you have a fully packaged Darcy AI application! The next step is to learn how to deploy. Follow the Deployment Guide to learn how to deploy your packaged Darcy AI apps.","title":"Next step is to deploy your Darcy AI application"},{"location":"pipeline/","text":"darcyai.pipeline Pipeline Objects class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\") num_of_edge_tpus def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus() add_perceptor def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: int = 0, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_before def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_after def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_parallel_perceptor def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) update_input_stream def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) remove_output_stream def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\") stop def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop() run def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run() get_pom def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom() get_current_pulse_number def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number() get_latest_input def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input() get_historical_input def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1) get_input_history def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history() get_historical_pom def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1) get_pom_history def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history() run_perceptor def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True) get_graph def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph() get_all_performance_metrics def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics() get_pulse_performance_metrics def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1) get_perceptor_performance_metrics def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1) set_perceptor_config def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1) get_perceptor_config def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\") set_output_stream_config def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1) get_output_stream_config def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"Pipeline"},{"location":"pipeline/#darcyaipipeline","text":"","title":"darcyai.pipeline"},{"location":"pipeline/#pipeline-objects","text":"class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\")","title":"Pipeline Objects"},{"location":"pipeline/#num_of_edge_tpus","text":"def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus()","title":"num_of_edge_tpus"},{"location":"pipeline/#add_perceptor","text":"def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: int = 0, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor"},{"location":"pipeline/#add_perceptor_before","text":"def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_before"},{"location":"pipeline/#add_perceptor_after","text":"def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to 0 . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_after"},{"location":"pipeline/#add_parallel_perceptor","text":"def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: int = 0, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_parallel_perceptor"},{"location":"pipeline/#update_input_stream","text":"def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"})","title":"update_input_stream"},{"location":"pipeline/#remove_output_stream","text":"def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\")","title":"remove_output_stream"},{"location":"pipeline/#stop","text":"def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop()","title":"stop"},{"location":"pipeline/#run","text":"def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run()","title":"run"},{"location":"pipeline/#get_pom","text":"def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom()","title":"get_pom"},{"location":"pipeline/#get_current_pulse_number","text":"def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number()","title":"get_current_pulse_number"},{"location":"pipeline/#get_latest_input","text":"def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input()","title":"get_latest_input"},{"location":"pipeline/#get_historical_input","text":"def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1)","title":"get_historical_input"},{"location":"pipeline/#get_input_history","text":"def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history()","title":"get_input_history"},{"location":"pipeline/#get_historical_pom","text":"def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1)","title":"get_historical_pom"},{"location":"pipeline/#get_pom_history","text":"def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history()","title":"get_pom_history"},{"location":"pipeline/#run_perceptor","text":"def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True)","title":"run_perceptor"},{"location":"pipeline/#get_graph","text":"def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph()","title":"get_graph"},{"location":"pipeline/#get_all_performance_metrics","text":"def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics()","title":"get_all_performance_metrics"},{"location":"pipeline/#get_pulse_performance_metrics","text":"def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1)","title":"get_pulse_performance_metrics"},{"location":"pipeline/#get_perceptor_performance_metrics","text":"def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1)","title":"get_perceptor_performance_metrics"},{"location":"pipeline/#set_perceptor_config","text":"def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1)","title":"set_perceptor_config"},{"location":"pipeline/#get_perceptor_config","text":"def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\")","title":"get_perceptor_config"},{"location":"pipeline/#set_output_stream_config","text":"def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1)","title":"set_output_stream_config"},{"location":"pipeline/#get_output_stream_config","text":"def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"get_output_stream_config"},{"location":"raspberry-pi-setup/","text":"Setting up your Raspberry Pi for Darcy AI development Raspberry Pi boards are excellent for building and running Darcy AI applications. This guide will show you how to get your RPi ready for Darcy AI development so you can run and debug your applications. Hardware you will need Raspberry Pi board (Pi 4 with 2GB+ of RAM recommended) Video camera attached to the camera port Google Coral edge TPU (USB version attached to USB 3.0 port) Micro SD card with at least 16GB capacity (32GB+ recommended) Power supply 5 Volts with at least 3 Amps output for RPi4 (standard Raspberry Pi 4 power supply) 5 Volts with at least 2.5 Amps output for RPi3 (most power supplies will qualify) Pick your Raspberry Pi operating system and flash your SD card You can choose either a 32-bit or 64-bit Raspberry Pi OS. The instructions in this guide are the same for either operating system version, but note that using the newly released 64-bit version may result in some unforeseen issues because it is new and not all software packages in the ecosystem have been updated yet. Follow the official Raspberry Pi Foundation instructions for getting your operating system and flashing it onto your Micro SD card. We recommend installing the Raspberry Pi OS with Desktop option because many helpful software packages will already be installed for you. The Raspberry Pi SD card imager can be found here: https://www.raspberrypi.com/software/ The Raspberry Pi OS versions can be downloaded here if you want to flash your SD card without the RPi Imager software: https://www.raspberrypi.com/software/operating-systems/ Enable SSH on your Raspberry Pi Follow this guide to enable SSH on your Raspberry Pi board. Take these steps before you boot up the RPi for the first time by following the instructions in the first red box titled \"NOTE\" that begin with the words \"For headless setup\". https://www.raspberrypi.com/documentation/computers/remote-access.html#enabling-the-server Connect your Raspberry Pi to your network and the Internet You can connect your RPi board using either a wired Ethernet connection or using WiFi. If you are using a wired connection, simply connect an Ethernet cable to your Raspberry Pi port and proceed to the next step. If you are using WiFi, then you need to use one of the following approaches to add your WiFi network to your Raspberry Pi. Add your WiFi network via the Raspberry Pi desktop If you are using the Raspberry Pi desktop and you have your RPi connected to a monitor, keyboard, and mouse then all you need to do is click on the WiFi icon in the upper-right toolbar area and add your WiFi network. You will also want to open a terminal window so you can proceed with the next steps which use the command line. Add your WiFi network via the wpa_supplicant file If you are not using the Raspberry Pi desktop then you will need to add your WiFi network using the file called wpa_supplicant.conf . This file is found under the /etc/wpa_supplicant/ directory. You can either add this file to the SD card before you boot up the Raspberry Pi or you can edit the file directly if you have a monitor, keyboard, and mouse attached to your RPi. Follow the instructions provided by the Raspberry Pi Foundation to configure your wpa_supplicant.conf file. https://www.raspberrypi.com/documentation/computers/configuration.html#configuring-networking31 Find your Raspberry Pi board IP address You will need to access your Raspberry Pi board using its hostname or IP address on your local network. This is true even if you are using a monitor, keyboard, and mouse with your RPi. Follow the instructions provided by the Raspberry Pi Foundation to identify your RPi board on the network. https://www.raspberrypi.com/documentation/computers/remote-access.html#how-to-find-your-ip-address Enable legacy camera support if you are using the newest Raspberry Pi OS As of late 2021, the Raspberry Pi OS defaults to a camera configuration that is not compatible with many existing pieces of software. If you are using the newest Raspberry Pi OS, you must enable legacy camera support. Open a command line terminal or SSH into your Raspberry Pi and type sudo raspi-config . Then go to \u2018Interface Options\u2019 and then \u2018Legacy Camera\u2019. Reboot when you are done. Python version Python 3.9.x should already be installed in your Raspberry Pi. You can check the version by running the command python --version . You will need Python 3.5.x or greater to build with Darcy AI. Numpy package for Python You will need the Numpy package for Python. Version 1.19.x should already be installed. You can check the version with the command pip list | grep numpy which will search for numpy in the entire list of installed Python packages. If you do not have Numpy you can install it with the command pip install numpy . You should have Numpy 1.19.x or greater to build with Darcy AI. Pillow package for Python You will need the Pillow package for Python. Version 8.1.x should already be installed. You can check the version with the command pip list | grep Pillow . If you do not have Pillow installed, you can install it with pip install Pillow . You should have Pillow 8.1.x or greater to build with Darcy AI. imutils package for Python Install the imutils package for Python with the command pip install imutils . DarcyAI package for Python Install the DarcyAI pacakge for Python with the command pip install darcyai . Install Docker You will need the Docker container runtime and command line tools installed on your Raspberry Pi. Install Docker with the following commands. curl -fsSL https://get.docker.com -o get-docker.sh This first command will download a convenient install script from Docker. Now run that script. sudo sh get-docker.sh Once Docker is installed, you need to add your standard user pi to the docker group so you can execute Docker commands without using sudo . sudo usermod -aG docker pi Install Google Coral software libraries You need the PyCoral and Edge TPU runtime software libraries installed on your Raspberry Pi. Follow the guide provided by Google and use the sections marked as \"on Linux\". https://coral.ai/docs/accelerator/get-started/#requirements Build OpenCV on your Raspberry Pi This step is the most time consuming. Fortunately, if you use the convenient script provided by Q-Engineering, you should find it fairly easy to perform the build and installation of this very important piece of software. Start by making sure you have a large enough swap file. If you are using a Raspberry Pi 4 with 8GB of RAM then you might not need this step. A larger swap file allocation allows the RPi to use disk space in place of RAM. A lot of RAM is needed to build OpenCV. After you follow the swap file setup steps, you will find the convenient build script about one third down the page. Expect to spend a couple of hours building OpenCV on your Raspberry Pi. Once this step is completed, you will have a modern version of OpenCV that is optimized for your Raspberry Pi board. This is worth doing, as you will use OpenCV in almost every computer vision application. Version 4.5.5 of OpenCV is recommended. If you encounter a crash at the 99% build progress level, try changing the installation script to use make -j2 instead of make -j4 . It will take a little longer but should prevent the crash. https://qengineering.eu/install-opencv-4.5-on-raspberry-pi-4.html Reboot your Raspberry Pi You will need to reboot your Raspberry Pi after finishing all of these installation steps. Make sure your camera and your Google Coral USB device are attached to your RPi when you reboot. sudo reboot Check for Google Coral USB device(s) You can see which devices are attached to your Raspberry Pi using the command lsusb . This will display a list of devices. Look for an item that shows Global Unichip Corp. . You will have more than one of these items if you have more than one Google Coral USB device attached. You will notice that once you start running a Darcy AI application, the value shown by lsusb will change to Google, Inc. . This is normal and expected and whenever you reboot your Raspberry Pi it will return to Global Unichip Corp. temporarily. Run the system check script Run the script called check.bash to scan your Raspberry Pi and make sure everything is looking good. If you receive an error, use the error message to pinpoint which step needs to be completed or fixed. Start building your AI applications with Darcy Go to the Build Guide and get started building with Darcy AI!","title":"Raspberry Pi Setup"},{"location":"raspberry-pi-setup/#setting-up-your-raspberry-pi-for-darcy-ai-development","text":"Raspberry Pi boards are excellent for building and running Darcy AI applications. This guide will show you how to get your RPi ready for Darcy AI development so you can run and debug your applications.","title":"Setting up your Raspberry Pi for Darcy AI development"},{"location":"raspberry-pi-setup/#hardware-you-will-need","text":"Raspberry Pi board (Pi 4 with 2GB+ of RAM recommended) Video camera attached to the camera port Google Coral edge TPU (USB version attached to USB 3.0 port) Micro SD card with at least 16GB capacity (32GB+ recommended) Power supply 5 Volts with at least 3 Amps output for RPi4 (standard Raspberry Pi 4 power supply) 5 Volts with at least 2.5 Amps output for RPi3 (most power supplies will qualify)","title":"Hardware you will need"},{"location":"raspberry-pi-setup/#pick-your-raspberry-pi-operating-system-and-flash-your-sd-card","text":"You can choose either a 32-bit or 64-bit Raspberry Pi OS. The instructions in this guide are the same for either operating system version, but note that using the newly released 64-bit version may result in some unforeseen issues because it is new and not all software packages in the ecosystem have been updated yet. Follow the official Raspberry Pi Foundation instructions for getting your operating system and flashing it onto your Micro SD card. We recommend installing the Raspberry Pi OS with Desktop option because many helpful software packages will already be installed for you. The Raspberry Pi SD card imager can be found here: https://www.raspberrypi.com/software/ The Raspberry Pi OS versions can be downloaded here if you want to flash your SD card without the RPi Imager software: https://www.raspberrypi.com/software/operating-systems/","title":"Pick your Raspberry Pi operating system and flash your SD card"},{"location":"raspberry-pi-setup/#enable-ssh-on-your-raspberry-pi","text":"Follow this guide to enable SSH on your Raspberry Pi board. Take these steps before you boot up the RPi for the first time by following the instructions in the first red box titled \"NOTE\" that begin with the words \"For headless setup\". https://www.raspberrypi.com/documentation/computers/remote-access.html#enabling-the-server","title":"Enable SSH on your Raspberry Pi"},{"location":"raspberry-pi-setup/#connect-your-raspberry-pi-to-your-network-and-the-internet","text":"You can connect your RPi board using either a wired Ethernet connection or using WiFi. If you are using a wired connection, simply connect an Ethernet cable to your Raspberry Pi port and proceed to the next step. If you are using WiFi, then you need to use one of the following approaches to add your WiFi network to your Raspberry Pi.","title":"Connect your Raspberry Pi to your network and the Internet"},{"location":"raspberry-pi-setup/#add-your-wifi-network-via-the-raspberry-pi-desktop","text":"If you are using the Raspberry Pi desktop and you have your RPi connected to a monitor, keyboard, and mouse then all you need to do is click on the WiFi icon in the upper-right toolbar area and add your WiFi network. You will also want to open a terminal window so you can proceed with the next steps which use the command line.","title":"Add your WiFi network via the Raspberry Pi desktop"},{"location":"raspberry-pi-setup/#add-your-wifi-network-via-the-wpa_supplicant-file","text":"If you are not using the Raspberry Pi desktop then you will need to add your WiFi network using the file called wpa_supplicant.conf . This file is found under the /etc/wpa_supplicant/ directory. You can either add this file to the SD card before you boot up the Raspberry Pi or you can edit the file directly if you have a monitor, keyboard, and mouse attached to your RPi. Follow the instructions provided by the Raspberry Pi Foundation to configure your wpa_supplicant.conf file. https://www.raspberrypi.com/documentation/computers/configuration.html#configuring-networking31","title":"Add your WiFi network via the wpa_supplicant file"},{"location":"raspberry-pi-setup/#find-your-raspberry-pi-board-ip-address","text":"You will need to access your Raspberry Pi board using its hostname or IP address on your local network. This is true even if you are using a monitor, keyboard, and mouse with your RPi. Follow the instructions provided by the Raspberry Pi Foundation to identify your RPi board on the network. https://www.raspberrypi.com/documentation/computers/remote-access.html#how-to-find-your-ip-address","title":"Find your Raspberry Pi board IP address"},{"location":"raspberry-pi-setup/#enable-legacy-camera-support-if-you-are-using-the-newest-raspberry-pi-os","text":"As of late 2021, the Raspberry Pi OS defaults to a camera configuration that is not compatible with many existing pieces of software. If you are using the newest Raspberry Pi OS, you must enable legacy camera support. Open a command line terminal or SSH into your Raspberry Pi and type sudo raspi-config . Then go to \u2018Interface Options\u2019 and then \u2018Legacy Camera\u2019. Reboot when you are done.","title":"Enable legacy camera support if you are using the newest Raspberry Pi OS"},{"location":"raspberry-pi-setup/#python-version","text":"Python 3.9.x should already be installed in your Raspberry Pi. You can check the version by running the command python --version . You will need Python 3.5.x or greater to build with Darcy AI.","title":"Python version"},{"location":"raspberry-pi-setup/#numpy-package-for-python","text":"You will need the Numpy package for Python. Version 1.19.x should already be installed. You can check the version with the command pip list | grep numpy which will search for numpy in the entire list of installed Python packages. If you do not have Numpy you can install it with the command pip install numpy . You should have Numpy 1.19.x or greater to build with Darcy AI.","title":"Numpy package for Python"},{"location":"raspberry-pi-setup/#pillow-package-for-python","text":"You will need the Pillow package for Python. Version 8.1.x should already be installed. You can check the version with the command pip list | grep Pillow . If you do not have Pillow installed, you can install it with pip install Pillow . You should have Pillow 8.1.x or greater to build with Darcy AI.","title":"Pillow package for Python"},{"location":"raspberry-pi-setup/#imutils-package-for-python","text":"Install the imutils package for Python with the command pip install imutils .","title":"imutils package for Python"},{"location":"raspberry-pi-setup/#darcyai-package-for-python","text":"Install the DarcyAI pacakge for Python with the command pip install darcyai .","title":"DarcyAI package for Python"},{"location":"raspberry-pi-setup/#install-docker","text":"You will need the Docker container runtime and command line tools installed on your Raspberry Pi. Install Docker with the following commands. curl -fsSL https://get.docker.com -o get-docker.sh This first command will download a convenient install script from Docker. Now run that script. sudo sh get-docker.sh Once Docker is installed, you need to add your standard user pi to the docker group so you can execute Docker commands without using sudo . sudo usermod -aG docker pi","title":"Install Docker"},{"location":"raspberry-pi-setup/#install-google-coral-software-libraries","text":"You need the PyCoral and Edge TPU runtime software libraries installed on your Raspberry Pi. Follow the guide provided by Google and use the sections marked as \"on Linux\". https://coral.ai/docs/accelerator/get-started/#requirements","title":"Install Google Coral software libraries"},{"location":"raspberry-pi-setup/#build-opencv-on-your-raspberry-pi","text":"This step is the most time consuming. Fortunately, if you use the convenient script provided by Q-Engineering, you should find it fairly easy to perform the build and installation of this very important piece of software. Start by making sure you have a large enough swap file. If you are using a Raspberry Pi 4 with 8GB of RAM then you might not need this step. A larger swap file allocation allows the RPi to use disk space in place of RAM. A lot of RAM is needed to build OpenCV. After you follow the swap file setup steps, you will find the convenient build script about one third down the page. Expect to spend a couple of hours building OpenCV on your Raspberry Pi. Once this step is completed, you will have a modern version of OpenCV that is optimized for your Raspberry Pi board. This is worth doing, as you will use OpenCV in almost every computer vision application. Version 4.5.5 of OpenCV is recommended. If you encounter a crash at the 99% build progress level, try changing the installation script to use make -j2 instead of make -j4 . It will take a little longer but should prevent the crash. https://qengineering.eu/install-opencv-4.5-on-raspberry-pi-4.html","title":"Build OpenCV on your Raspberry Pi"},{"location":"raspberry-pi-setup/#reboot-your-raspberry-pi","text":"You will need to reboot your Raspberry Pi after finishing all of these installation steps. Make sure your camera and your Google Coral USB device are attached to your RPi when you reboot. sudo reboot","title":"Reboot your Raspberry Pi"},{"location":"raspberry-pi-setup/#check-for-google-coral-usb-devices","text":"You can see which devices are attached to your Raspberry Pi using the command lsusb . This will display a list of devices. Look for an item that shows Global Unichip Corp. . You will have more than one of these items if you have more than one Google Coral USB device attached. You will notice that once you start running a Darcy AI application, the value shown by lsusb will change to Google, Inc. . This is normal and expected and whenever you reboot your Raspberry Pi it will return to Global Unichip Corp. temporarily.","title":"Check for Google Coral USB device(s)"},{"location":"raspberry-pi-setup/#run-the-system-check-script","text":"Run the script called check.bash to scan your Raspberry Pi and make sure everything is looking good. If you receive an error, use the error message to pinpoint which step needs to be completed or fixed.","title":"Run the system check script"},{"location":"raspberry-pi-setup/#start-building-your-ai-applications-with-darcy","text":"Go to the Build Guide and get started building with Darcy AI!","title":"Start building your AI applications with Darcy"},{"location":"serializable/","text":"darcyai.serializable Serializable Objects class Serializable() Base class for all serializable objects. serialize def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"Serializable"},{"location":"serializable/#darcyaiserializable","text":"","title":"darcyai.serializable"},{"location":"serializable/#serializable-objects","text":"class Serializable() Base class for all serializable objects.","title":"Serializable Objects"},{"location":"serializable/#serialize","text":"def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"serialize"},{"location":"streamdata/","text":"darcyai.stream_data StreamData Objects class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data. serialize def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"StreamData"},{"location":"streamdata/#darcyaistream_data","text":"","title":"darcyai.stream_data"},{"location":"streamdata/#streamdata-objects","text":"class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data.","title":"StreamData Objects"},{"location":"streamdata/#serialize","text":"def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"serialize"},{"location":"terminology/","text":"Darcy AI platform terminology guide Before you begin building with the Darcy AI SDK or testing its functionality, you need to understand the terminology and basic architecture. This document will give you an understanding quickly. Engine The Darcy AI engine is the part that runs the AI computations and manages the system resources such as memory and CPU threads. It can be considered the core \u201cbackend code\u201d of the Darcy AI SDK. As a developer, you do not need to interact with the engine directly. You only need to use the provided interfaces in the API as described in the documentation. Pipeline Every Darcy AI application is allowed one pipeline. A Darcy AI pipeline is the sequenced set of AI processes that does the real work in the application. The Darcy AI pipeline code object is one of the main objects that you will interact with as a developer. It contains many important methods and the AI processing starts when the \u201crun()\u201d method is called. Perceptor A Darcy AI perceptor is a code module that integrates raw AI processing with CPU code to make an easy-to-use semantic interface for the underlying AI output. Perceptors are built by developers who understand AI programming but are used by AI application developers who want to leverage the perceptor abilities. This frees AI application developers from needing to become AI experts and opens a perceptor library ecosystem. Perception Object Model (POM) Similar to the Document Object Model (DOM) that is found in web browsers, the Perception Object Model (POM) is a data tree structure found in Darcy AI applications. The POM is the place where the outputs of each pipeline step are stored. The POM is available to Darcy AI application developers at each pipeline step, when a whole pipeline cycle has been completed, and at any point when the developer desires to interact with it. The POM also contains a history of all AI raw inputs and processing results. Input Stream A Darcy AI input stream is the source data that is used for AI processing. Because Darcy\u2019s \u201csenses\u201d can be expanded to include any source of data, an input stream code object is used to encapsulate the processing that is done to prepare incoming data for AI workloads. An example of an input stream code library is one that captures the frames of video from a camera and also merges the thermal camera data with each frame, even though the two cameras provide data at different rates. An input stream is attached to a pipeline by you, the Darcy AI application developer. Output Stream A Darcy AI output stream is a code library that receives the data from the pipeline processing and produces a useful output, such as a video display or a CSV file. Many output streams can be attached to a single pipeline by you, the Darcy AI application developer. Callback For every step in the Darcy AI application processing, work is needed to format and produce business value from incoming data and outgoing data. The way that Darcy allows developers to do this work is to have their code processed by Darcy when the time is right. This is called a \u201ccallback\u201d and it is a well-known pattern of software development in JavaScript and other languages. By using callbacks, developers can focus on just the pieces of code that relate to their actual application and know that Darcy will run their code for them. Frame, Cycle, or Pulse Every complete trip through a Darcy AI pipeline is called a frame. It can also be called a cycle or a pulse. Initialization In order to allow Darcy to start doing AI processing, some foundational settings must be chosen and some basic requirements must be met, such as providing an input stream. Then the Darcy AI pipeline needs to be started so the application can run. These steps are called the Darcy AI initialization and they must be performed by the developer in every application. Docker Base Image There are many software packages and libraries that Darcy AI applications need in order to build and run properly. Asking you, the developer, to know and understand these dependencies would slow you down and cause you unecessary complexity. To circumvent this problem, the required software is bundled ahead of time in easy-to-use base images that are Docker containers. Because they are already Docker containers, you can make your application Docker containers easily by starting from one of the provided base container images. Performance Metrics Darcy tracks system performance when doing AI processing. Each trip through the pipeline steps is measured, along with the individual pipeline steps. Darcy AI application developers can request this performance data in their application, which allows for benchmarking, profiling, and innovative displays that show how fast each part of Darcy\u2019s work is being done. AI Model The actual AI neural network processing is done using AI models. An AI model is a stored image of a neural net that was built during an AI training or retraining process. Most developers use AI models that already exist and were created by someone else. Darcy AI perceptors contain AI models and make them easier to use. Most Darcy AI application developers do not need to use AI models directly because of the perceptor architecture.","title":"Terminology"},{"location":"terminology/#darcy-ai-platform-terminology-guide","text":"Before you begin building with the Darcy AI SDK or testing its functionality, you need to understand the terminology and basic architecture. This document will give you an understanding quickly.","title":"Darcy AI platform terminology guide"},{"location":"terminology/#engine","text":"The Darcy AI engine is the part that runs the AI computations and manages the system resources such as memory and CPU threads. It can be considered the core \u201cbackend code\u201d of the Darcy AI SDK. As a developer, you do not need to interact with the engine directly. You only need to use the provided interfaces in the API as described in the documentation.","title":"Engine"},{"location":"terminology/#pipeline","text":"Every Darcy AI application is allowed one pipeline. A Darcy AI pipeline is the sequenced set of AI processes that does the real work in the application. The Darcy AI pipeline code object is one of the main objects that you will interact with as a developer. It contains many important methods and the AI processing starts when the \u201crun()\u201d method is called.","title":"Pipeline"},{"location":"terminology/#perceptor","text":"A Darcy AI perceptor is a code module that integrates raw AI processing with CPU code to make an easy-to-use semantic interface for the underlying AI output. Perceptors are built by developers who understand AI programming but are used by AI application developers who want to leverage the perceptor abilities. This frees AI application developers from needing to become AI experts and opens a perceptor library ecosystem.","title":"Perceptor"},{"location":"terminology/#perception-object-model-pom","text":"Similar to the Document Object Model (DOM) that is found in web browsers, the Perception Object Model (POM) is a data tree structure found in Darcy AI applications. The POM is the place where the outputs of each pipeline step are stored. The POM is available to Darcy AI application developers at each pipeline step, when a whole pipeline cycle has been completed, and at any point when the developer desires to interact with it. The POM also contains a history of all AI raw inputs and processing results.","title":"Perception Object Model (POM)"},{"location":"terminology/#input-stream","text":"A Darcy AI input stream is the source data that is used for AI processing. Because Darcy\u2019s \u201csenses\u201d can be expanded to include any source of data, an input stream code object is used to encapsulate the processing that is done to prepare incoming data for AI workloads. An example of an input stream code library is one that captures the frames of video from a camera and also merges the thermal camera data with each frame, even though the two cameras provide data at different rates. An input stream is attached to a pipeline by you, the Darcy AI application developer.","title":"Input Stream"},{"location":"terminology/#output-stream","text":"A Darcy AI output stream is a code library that receives the data from the pipeline processing and produces a useful output, such as a video display or a CSV file. Many output streams can be attached to a single pipeline by you, the Darcy AI application developer.","title":"Output Stream"},{"location":"terminology/#callback","text":"For every step in the Darcy AI application processing, work is needed to format and produce business value from incoming data and outgoing data. The way that Darcy allows developers to do this work is to have their code processed by Darcy when the time is right. This is called a \u201ccallback\u201d and it is a well-known pattern of software development in JavaScript and other languages. By using callbacks, developers can focus on just the pieces of code that relate to their actual application and know that Darcy will run their code for them.","title":"Callback"},{"location":"terminology/#frame-cycle-or-pulse","text":"Every complete trip through a Darcy AI pipeline is called a frame. It can also be called a cycle or a pulse.","title":"Frame, Cycle, or Pulse"},{"location":"terminology/#initialization","text":"In order to allow Darcy to start doing AI processing, some foundational settings must be chosen and some basic requirements must be met, such as providing an input stream. Then the Darcy AI pipeline needs to be started so the application can run. These steps are called the Darcy AI initialization and they must be performed by the developer in every application.","title":"Initialization"},{"location":"terminology/#docker-base-image","text":"There are many software packages and libraries that Darcy AI applications need in order to build and run properly. Asking you, the developer, to know and understand these dependencies would slow you down and cause you unecessary complexity. To circumvent this problem, the required software is bundled ahead of time in easy-to-use base images that are Docker containers. Because they are already Docker containers, you can make your application Docker containers easily by starting from one of the provided base container images.","title":"Docker Base Image"},{"location":"terminology/#performance-metrics","text":"Darcy tracks system performance when doing AI processing. Each trip through the pipeline steps is measured, along with the individual pipeline steps. Darcy AI application developers can request this performance data in their application, which allows for benchmarking, profiling, and innovative displays that show how fast each part of Darcy\u2019s work is being done.","title":"Performance Metrics"},{"location":"terminology/#ai-model","text":"The actual AI neural network processing is done using AI models. An AI model is a stored image of a neural net that was built during an AI training or retraining process. Most developers use AI models that already exist and were created by someone else. Darcy AI perceptors contain AI models and make them easier to use. Most Darcy AI application developers do not need to use AI models directly because of the perceptor architecture.","title":"AI Model"},{"location":"windows-setup/","text":"Setting up your Windows environment for Darcy AI development You can use any Windows computer to build Darcy AI applications. After you finish setting up your computer as a development environment, you will be able to write, test, and debug your Darcy AI apps. Then you can package your code and deploy the applications to any edge device! What you will need Windows laptop or desktop A video camera that you want to use (a built-in webcam or USB webcam works very well for this purpose) 5GB or more of free disk space to accommodate code libraries and application container images Any IDE software that will allow you to write and debug Python (use your favorite IDE) Install Python 3.6.9 or greater If you do not already have Python version 3.6.9 or greater, you will need to install it now. Darcy AI requires this version of Python or higher. Note that Python 2.x versions are also not compatible with Darcy AI. You need Python 3 and the darcyai library will not install with versions below 3.6.9. Download and install the latest or your preferred version of Python3 from Python.org directly https://www.python.org/downloads/windows/ . If you have both Python 2.x versions and Python 3.x versions on your Windows machine, you may need to use python3 and pip3 for all of the commands instead of just python and pip . You can make adjustments to how Python gets launched on your Windows computer if you want by following this guide https://docs.python.org/3/using/windows.html#python-launcher-for-windows . Install OpenCV Install the OpenCV package for Python with the command py -m pip install opencv-python Install the Pillow library Install the Pillow package for Python with the command py -m pip install Pillow . Install the Numpy library Install the numpy package for Python with the command py -m pip install numpy . Install the Imutils library Install the imutils package for Python with the command py -m pip install imutils . Install the DarcyAI library Install the Darcy AI library for Python with the command py -m pip install darcyai . Install Docker for Windows If you don't already have Docker on your Mac, install it now by following the official instructions https://docs.docker.com/desktop/windows/install/ . After you have installed Docker, you can use docker commands on the command line. You will be using these commands to package your Darcy AI applications for deployment, including deploying to edge devices that are a different CPU architecture than your Windows machine! To make sure you can use the latest Docker build commands like buildx you can enable experimental features using this guide https://docs.docker.com/desktop/windows/#command-line . This will tell Docker to allow use of the latest tools which will save you a lot of time when packaging your apps! Install TensorFlow Install TensorFlow for Python with the command py -m pip install tensorflow Start building your AI applications with Darcy Go to the Build Guide and get started building with Darcy AI!","title":"Windows Setup"},{"location":"windows-setup/#setting-up-your-windows-environment-for-darcy-ai-development","text":"You can use any Windows computer to build Darcy AI applications. After you finish setting up your computer as a development environment, you will be able to write, test, and debug your Darcy AI apps. Then you can package your code and deploy the applications to any edge device!","title":"Setting up your Windows environment for Darcy AI development"},{"location":"windows-setup/#what-you-will-need","text":"Windows laptop or desktop A video camera that you want to use (a built-in webcam or USB webcam works very well for this purpose) 5GB or more of free disk space to accommodate code libraries and application container images Any IDE software that will allow you to write and debug Python (use your favorite IDE)","title":"What you will need"},{"location":"windows-setup/#install-python-369-or-greater","text":"If you do not already have Python version 3.6.9 or greater, you will need to install it now. Darcy AI requires this version of Python or higher. Note that Python 2.x versions are also not compatible with Darcy AI. You need Python 3 and the darcyai library will not install with versions below 3.6.9. Download and install the latest or your preferred version of Python3 from Python.org directly https://www.python.org/downloads/windows/ . If you have both Python 2.x versions and Python 3.x versions on your Windows machine, you may need to use python3 and pip3 for all of the commands instead of just python and pip . You can make adjustments to how Python gets launched on your Windows computer if you want by following this guide https://docs.python.org/3/using/windows.html#python-launcher-for-windows .","title":"Install Python 3.6.9 or greater"},{"location":"windows-setup/#install-opencv","text":"Install the OpenCV package for Python with the command py -m pip install opencv-python","title":"Install OpenCV"},{"location":"windows-setup/#install-the-pillow-library","text":"Install the Pillow package for Python with the command py -m pip install Pillow .","title":"Install the Pillow library"},{"location":"windows-setup/#install-the-numpy-library","text":"Install the numpy package for Python with the command py -m pip install numpy .","title":"Install the Numpy library"},{"location":"windows-setup/#install-the-imutils-library","text":"Install the imutils package for Python with the command py -m pip install imutils .","title":"Install the Imutils library"},{"location":"windows-setup/#install-the-darcyai-library","text":"Install the Darcy AI library for Python with the command py -m pip install darcyai .","title":"Install the DarcyAI library"},{"location":"windows-setup/#install-docker-for-windows","text":"If you don't already have Docker on your Mac, install it now by following the official instructions https://docs.docker.com/desktop/windows/install/ . After you have installed Docker, you can use docker commands on the command line. You will be using these commands to package your Darcy AI applications for deployment, including deploying to edge devices that are a different CPU architecture than your Windows machine! To make sure you can use the latest Docker build commands like buildx you can enable experimental features using this guide https://docs.docker.com/desktop/windows/#command-line . This will tell Docker to allow use of the latest tools which will save you a lot of time when packaging your apps!","title":"Install Docker for Windows"},{"location":"windows-setup/#install-tensorflow","text":"Install TensorFlow for Python with the command py -m pip install tensorflow","title":"Install TensorFlow"},{"location":"windows-setup/#start-building-your-ai-applications-with-darcy","text":"Go to the Build Guide and get started building with Darcy AI!","title":"Start building your AI applications with Darcy"},{"location":"input-streams/camerastream/","text":"darcyai.input.camera_stream CameraStream Objects class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\") stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream() get_video_inputs @staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"CameraStream"},{"location":"input-streams/camerastream/#darcyaiinputcamera_stream","text":"","title":"darcyai.input.camera_stream"},{"location":"input-streams/camerastream/#camerastream-objects","text":"class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\")","title":"CameraStream Objects"},{"location":"input-streams/camerastream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop()","title":"stop"},{"location":"input-streams/camerastream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream()","title":"stream"},{"location":"input-streams/camerastream/#get_video_inputs","text":"@staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"get_video_inputs"},{"location":"input-streams/inputmultistream/","text":"darcyai.input.input_multi_stream InputMultiStream Objects class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback) remove_stream def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\") get_stream def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\") add_stream def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) stream def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream() stop def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"InputMultiStream"},{"location":"input-streams/inputmultistream/#darcyaiinputinput_multi_stream","text":"","title":"darcyai.input.input_multi_stream"},{"location":"input-streams/inputmultistream/#inputmultistream-objects","text":"class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback)","title":"InputMultiStream Objects"},{"location":"input-streams/inputmultistream/#remove_stream","text":"def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\")","title":"remove_stream"},{"location":"input-streams/inputmultistream/#get_stream","text":"def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\")","title":"get_stream"},{"location":"input-streams/inputmultistream/#add_stream","text":"def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera)","title":"add_stream"},{"location":"input-streams/inputmultistream/#stream","text":"def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream()","title":"stream"},{"location":"input-streams/inputmultistream/#stop","text":"def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"stop"},{"location":"input-streams/inputstream/","text":"darcyai.input.input_stream InputStream Objects class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass stream def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input. stop def stop() -> None Stops the stream.","title":"InputStream"},{"location":"input-streams/inputstream/#darcyaiinputinput_stream","text":"","title":"darcyai.input.input_stream"},{"location":"input-streams/inputstream/#inputstream-objects","text":"class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass","title":"InputStream Objects"},{"location":"input-streams/inputstream/#stream","text":"def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input.","title":"stream"},{"location":"input-streams/inputstream/#stop","text":"def stop() -> None Stops the stream.","title":"stop"},{"location":"input-streams/videofilestream/","text":"darcyai.input.video_file_stream VideoFileStream Objects class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"VideoFileStream"},{"location":"input-streams/videofilestream/#darcyaiinputvideo_file_stream","text":"","title":"darcyai.input.video_file_stream"},{"location":"input-streams/videofilestream/#videofilestream-objects","text":"class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True)","title":"VideoFileStream Objects"},{"location":"input-streams/videofilestream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop()","title":"stop"},{"location":"input-streams/videofilestream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"stream"},{"location":"input-streams/videostreamdata/","text":"darcyai.input.video_stream_data VideoStreamData Objects class VideoStreamData(StreamData) StreamData representation of video frames serialize def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"VideoStreamData"},{"location":"input-streams/videostreamdata/#darcyaiinputvideo_stream_data","text":"","title":"darcyai.input.video_stream_data"},{"location":"input-streams/videostreamdata/#videostreamdata-objects","text":"class VideoStreamData(StreamData) StreamData representation of video frames","title":"VideoStreamData Objects"},{"location":"input-streams/videostreamdata/#serialize","text":"def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"serialize"},{"location":"output-streams/csvoutputstream/","text":"darcyai.output.csv_output_stream CSVOutputStream Objects class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) write def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]]) close def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"CSVOutputStream"},{"location":"output-streams/csvoutputstream/#darcyaioutputcsv_output_stream","text":"","title":"darcyai.output.csv_output_stream"},{"location":"output-streams/csvoutputstream/#csvoutputstream-objects","text":"class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0)","title":"CSVOutputStream Objects"},{"location":"output-streams/csvoutputstream/#write","text":"def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]])","title":"write"},{"location":"output-streams/csvoutputstream/#close","text":"def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"close"},{"location":"output-streams/jsonoutputstream/","text":"darcyai.output.json_output_stream JSONOutputStream Objects class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) write def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"}) close def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"JSONOutputStream"},{"location":"output-streams/jsonoutputstream/#darcyaioutputjson_output_stream","text":"","title":"darcyai.output.json_output_stream"},{"location":"output-streams/jsonoutputstream/#jsonoutputstream-objects","text":"class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0)","title":"JSONOutputStream Objects"},{"location":"output-streams/jsonoutputstream/#write","text":"def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"})","title":"write"},{"location":"output-streams/jsonoutputstream/#close","text":"def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/","text":"darcyai.output.live_feed_stream LiveFeedStream Objects class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) write def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame) get_fps def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps() set_fps def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30) get_quality def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality() set_quality def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50) close def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close() get_latest_frame def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"LiveFeedStream"},{"location":"output-streams/livefeedstream/#darcyaioutputlive_feed_stream","text":"","title":"darcyai.output.live_feed_stream"},{"location":"output-streams/livefeedstream/#livefeedstream-objects","text":"class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100)","title":"LiveFeedStream Objects"},{"location":"output-streams/livefeedstream/#write","text":"def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame)","title":"write"},{"location":"output-streams/livefeedstream/#get_fps","text":"def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps()","title":"get_fps"},{"location":"output-streams/livefeedstream/#set_fps","text":"def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30)","title":"set_fps"},{"location":"output-streams/livefeedstream/#get_quality","text":"def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality()","title":"get_quality"},{"location":"output-streams/livefeedstream/#set_quality","text":"def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50)","title":"set_quality"},{"location":"output-streams/livefeedstream/#close","text":"def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/#get_latest_frame","text":"def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"get_latest_frame"},{"location":"output-streams/outputstream/","text":"darcyai.output.output_stream OutputStream Objects class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass write def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream. close def close() -> None Closes the output stream. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"OutputStream"},{"location":"output-streams/outputstream/#darcyaioutputoutput_stream","text":"","title":"darcyai.output.output_stream"},{"location":"output-streams/outputstream/#outputstream-objects","text":"class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass","title":"OutputStream Objects"},{"location":"output-streams/outputstream/#write","text":"def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream.","title":"write"},{"location":"output-streams/outputstream/#close","text":"def close() -> None Closes the output stream.","title":"close"},{"location":"output-streams/outputstream/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"output-streams/outputstream/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"output-streams/outputstream/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"output-streams/restapistream/","text":"darcyai.output.rest_api_stream RestApiStream Objects class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) write def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"}) close def close() -> None Closes the output stream.","title":"RestApiStream"},{"location":"output-streams/restapistream/#darcyaioutputrest_api_stream","text":"","title":"darcyai.output.rest_api_stream"},{"location":"output-streams/restapistream/#restapistream-objects","text":"class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"})","title":"RestApiStream Objects"},{"location":"output-streams/restapistream/#write","text":"def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"})","title":"write"},{"location":"output-streams/restapistream/#close","text":"def close() -> None Closes the output stream.","title":"close"},{"location":"perceptors/class/","text":"darcyai.perceptor.detected_class Class Objects class Class() Class to store detected class information. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. __str__ def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"Class"},{"location":"perceptors/class/#darcyaiperceptordetected_class","text":"","title":"darcyai.perceptor.detected_class"},{"location":"perceptors/class/#class-objects","text":"class Class() Class to store detected class information. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class.","title":"Class Objects"},{"location":"perceptors/class/#__str__","text":"def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"__str__"},{"location":"perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(MultiPlatformPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. __init__ def __init__(processor_preference: dict, threshold: float, top_k: int = None, mean: float = 128.0, std: float = 128.0, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. mean ( float ): The mean of the image (Coral Edge TPU). std ( float ): The standard deviation of the image (Coral Edge TPU). quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.image_classification_perceptor import ImageClassificationPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", // The path to the labels file. }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { // A dictionary of labels. \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } image_classification_perceptor = ImageClassificationPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"ImageClassificationPerceptor"},{"location":"perceptors/imageclassificationperceptor/#darcyaiperceptorimage_classification_perceptor","text":"","title":"darcyai.perceptor.image_classification_perceptor"},{"location":"perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(MultiPlatformPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/imageclassificationperceptor/#__init__","text":"def __init__(processor_preference: dict, threshold: float, top_k: int = None, mean: float = 128.0, std: float = 128.0, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. mean ( float ): The mean of the image (Coral Edge TPU). std ( float ): The standard deviation of the image (Coral Edge TPU). quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.image_classification_perceptor import ImageClassificationPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", // The path to the labels file. }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { // A dictionary of labels. \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } image_classification_perceptor = ImageClassificationPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"__init__"},{"location":"perceptors/object/","text":"darcyai.perceptor.detected_object Object Objects class Object() Class for storing detected objects. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. xmin ( int ): The x-coordinate of the top left corner of the bounding box. ymin ( int ): The y-coordinate of the top left corner of the bounding box. xmax ( int ): The x-coordinate of the bottom right corner of the bounding box. ymax ( int ): The y-coordinate of the bottom right corner of the bounding box. __str__ def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"Object"},{"location":"perceptors/object/#darcyaiperceptordetected_object","text":"","title":"darcyai.perceptor.detected_object"},{"location":"perceptors/object/#object-objects","text":"class Object() Class for storing detected objects. Arguments class_id ( int ): The class ID. name ( str ): The class name. confidence ( float ): The confidence of the class. xmin ( int ): The x-coordinate of the top left corner of the bounding box. ymin ( int ): The y-coordinate of the top left corner of the bounding box. xmax ( int ): The x-coordinate of the bottom right corner of the bounding box. ymax ( int ): The y-coordinate of the bottom right corner of the bounding box.","title":"Object Objects"},{"location":"perceptors/object/#__str__","text":"def __str__() Returns a string representation of the class. Returns str : The string representation of the class.","title":"__str__"},{"location":"perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(MultiPlatformPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. __init__ def __init__(processor_preference: dict, threshold: float, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.object_detection_perceptor import ObjectDetectionPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } object_detection_perceptor = ObjectDetectionPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"ObjectDetectionPerceptor"},{"location":"perceptors/objectdetectionperceptor/#darcyaiperceptorobject_detection_perceptor","text":"","title":"darcyai.perceptor.object_detection_perceptor"},{"location":"perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(MultiPlatformPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/objectdetectionperceptor/#__init__","text":"def __init__(processor_preference: dict, threshold: float, quantized: bool = True, num_cpu_threads: int = 1) Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. threshold ( float ): The threshold for object detection. quantized ( bool ): Whether the model is quantized (CPU). num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. Example from darcyai.perceptor.object_detection_perceptor import ObjectDetectionPerceptor from darcyai.perceptor.processor import Processor processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels\": { \"label_1\": \"label_1_name\", \"label_2\": \"label_2_name\", }, }, } object_detection_perceptor = ObjectDetectionPerceptor( processor_preference=processor_preference, threshold=0.5, top_k=5)","title":"__init__"},{"location":"perceptors/peopleperceptor/","text":"darcyai.perceptor.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(MultiPlatformPerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/peopleperceptor/#darcyaiperceptorpeople_perceptor","text":"","title":"darcyai.perceptor.people_perceptor"},{"location":"perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(MultiPlatformPerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"perceptors/peoplepom/","text":"darcyai.perceptor.people_perceptor_pom PeoplePOM Objects class PeoplePOM(Serializable) People perceptor object model set_annotated_frame def set_annotated_frame(frame) Stores the frame annotated with the persons data Arguments frame ( np.array ): The annotated frame set_raw_frame def set_raw_frame(frame) Stores the frame with the people data Arguments frame ( np.array ): The raw frame set_people def set_people(people) Stores the detected people Arguments people ( list ): The detected people annotatedFrame def annotatedFrame() Returns the annotated frame Returns np.array : The annotated frame rawFrame def rawFrame() Returns the raw frame Returns np.array : The raw frame peopleCount def peopleCount() Returns the number of people detected Returns int : The number of people detected personInFront def personInFront() Returns the person in front of the camera Returns The person in front of the camera people def people() Returns the detected people Returns list : The detected people person def person(person_id) Returns the person with the given id Arguments person_id ( str ): The person id Returns dict : The person faceImage def faceImage(person_id) Returns the face image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The face image bodyImage def bodyImage(person_id) Returns the body image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The body image personImage def personImage(person_id) Returns the image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The person image faceSize def faceSize(person_id) Returns the size of the face of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the face bodySize def bodySize(person_id) Returns the size of the body of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the body wholeSize def wholeSize(person_id) Returns the size of the whole person with the given id Arguments person_id ( str ): The person id Returns int : The size of the whole person bodyPose def bodyPose(person_id) Returns the PoseNet data of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data bodyPoseHistory def bodyPoseHistory(person_id) Returns the PoseNet data history of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data travelPath def travelPath(person_id) Returns the travel path of the person with the given id Arguments person_id ( str ): The person id Returns The travel path timeInView def timeInView(person_id) Returns the time the person with the given id has been in view Arguments person_id ( str ): The person id Returns int : The time in view recentlyDepartedPeople def recentlyDepartedPeople() Returns the people that have recently departed Returns list : The people that have recently departed occludedPeople def occludedPeople() Returns the people that are occluded Returns list : The people that are occluded serialize def serialize() Returns the serialized data of the object Returns dict : The serialized data","title":"PeoplePOM"},{"location":"perceptors/peoplepom/#darcyaiperceptorpeople_perceptor_pom","text":"","title":"darcyai.perceptor.people_perceptor_pom"},{"location":"perceptors/peoplepom/#peoplepom-objects","text":"class PeoplePOM(Serializable) People perceptor object model","title":"PeoplePOM Objects"},{"location":"perceptors/peoplepom/#set_annotated_frame","text":"def set_annotated_frame(frame) Stores the frame annotated with the persons data Arguments frame ( np.array ): The annotated frame","title":"set_annotated_frame"},{"location":"perceptors/peoplepom/#set_raw_frame","text":"def set_raw_frame(frame) Stores the frame with the people data Arguments frame ( np.array ): The raw frame","title":"set_raw_frame"},{"location":"perceptors/peoplepom/#set_people","text":"def set_people(people) Stores the detected people Arguments people ( list ): The detected people","title":"set_people"},{"location":"perceptors/peoplepom/#annotatedframe","text":"def annotatedFrame() Returns the annotated frame Returns np.array : The annotated frame","title":"annotatedFrame"},{"location":"perceptors/peoplepom/#rawframe","text":"def rawFrame() Returns the raw frame Returns np.array : The raw frame","title":"rawFrame"},{"location":"perceptors/peoplepom/#peoplecount","text":"def peopleCount() Returns the number of people detected Returns int : The number of people detected","title":"peopleCount"},{"location":"perceptors/peoplepom/#personinfront","text":"def personInFront() Returns the person in front of the camera Returns The person in front of the camera","title":"personInFront"},{"location":"perceptors/peoplepom/#people","text":"def people() Returns the detected people Returns list : The detected people","title":"people"},{"location":"perceptors/peoplepom/#person","text":"def person(person_id) Returns the person with the given id Arguments person_id ( str ): The person id Returns dict : The person","title":"person"},{"location":"perceptors/peoplepom/#faceimage","text":"def faceImage(person_id) Returns the face image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The face image","title":"faceImage"},{"location":"perceptors/peoplepom/#bodyimage","text":"def bodyImage(person_id) Returns the body image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The body image","title":"bodyImage"},{"location":"perceptors/peoplepom/#personimage","text":"def personImage(person_id) Returns the image of the person with the given id Arguments person_id ( str ): The person id Returns np.array : The person image","title":"personImage"},{"location":"perceptors/peoplepom/#facesize","text":"def faceSize(person_id) Returns the size of the face of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the face","title":"faceSize"},{"location":"perceptors/peoplepom/#bodysize","text":"def bodySize(person_id) Returns the size of the body of the person with the given id Arguments person_id ( str ): The person id Returns int : The size of the body","title":"bodySize"},{"location":"perceptors/peoplepom/#wholesize","text":"def wholeSize(person_id) Returns the size of the whole person with the given id Arguments person_id ( str ): The person id Returns int : The size of the whole person","title":"wholeSize"},{"location":"perceptors/peoplepom/#bodypose","text":"def bodyPose(person_id) Returns the PoseNet data of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data","title":"bodyPose"},{"location":"perceptors/peoplepom/#bodyposehistory","text":"def bodyPoseHistory(person_id) Returns the PoseNet data history of the person with the given id Arguments person_id ( str ): The person id Returns The PoseNet data","title":"bodyPoseHistory"},{"location":"perceptors/peoplepom/#travelpath","text":"def travelPath(person_id) Returns the travel path of the person with the given id Arguments person_id ( str ): The person id Returns The travel path","title":"travelPath"},{"location":"perceptors/peoplepom/#timeinview","text":"def timeInView(person_id) Returns the time the person with the given id has been in view Arguments person_id ( str ): The person id Returns int : The time in view","title":"timeInView"},{"location":"perceptors/peoplepom/#recentlydepartedpeople","text":"def recentlyDepartedPeople() Returns the people that have recently departed Returns list : The people that have recently departed","title":"recentlyDepartedPeople"},{"location":"perceptors/peoplepom/#occludedpeople","text":"def occludedPeople() Returns the people that are occluded Returns list : The people that are occluded","title":"occludedPeople"},{"location":"perceptors/peoplepom/#serialize","text":"def serialize() Returns the serialized data of the object Returns dict : The serialized data","title":"serialize"},{"location":"perceptors/perceptionobjectmodel/","text":"darcyai.perception_object_model PerceptionObjectModel Objects class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object. set_value def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set. get_perceptor def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception. get_perceptors def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors. serialize def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model. set_input_data def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data. get_input_data def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data. set_pulse_number def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number. get_pulse_number def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number. set_pps def set_pps(pps: int) -> None Set the pps for the perception object model. Arguments pps ( int ): The pps. get_pps def get_pps() -> int Get the pps for the perception object model. Returns int : The pps.","title":"PerceptionObjectModel"},{"location":"perceptors/perceptionobjectmodel/#darcyaiperception_object_model","text":"","title":"darcyai.perception_object_model"},{"location":"perceptors/perceptionobjectmodel/#perceptionobjectmodel-objects","text":"class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object.","title":"PerceptionObjectModel Objects"},{"location":"perceptors/perceptionobjectmodel/#set_value","text":"def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set.","title":"set_value"},{"location":"perceptors/perceptionobjectmodel/#get_perceptor","text":"def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception.","title":"get_perceptor"},{"location":"perceptors/perceptionobjectmodel/#get_perceptors","text":"def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors.","title":"get_perceptors"},{"location":"perceptors/perceptionobjectmodel/#serialize","text":"def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model.","title":"serialize"},{"location":"perceptors/perceptionobjectmodel/#set_input_data","text":"def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data.","title":"set_input_data"},{"location":"perceptors/perceptionobjectmodel/#get_input_data","text":"def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data.","title":"get_input_data"},{"location":"perceptors/perceptionobjectmodel/#set_pulse_number","text":"def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number.","title":"set_pulse_number"},{"location":"perceptors/perceptionobjectmodel/#get_pulse_number","text":"def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number.","title":"get_pulse_number"},{"location":"perceptors/perceptionobjectmodel/#set_pps","text":"def set_pps(pps: int) -> None Set the pps for the perception object model. Arguments pps ( int ): The pps.","title":"set_pps"},{"location":"perceptors/perceptionobjectmodel/#get_pps","text":"def get_pps() -> int Get the pps for the perception object model. Returns int : The pps.","title":"get_pps"},{"location":"perceptors/perceptor-utils/","text":"darcyai.perceptor.perceptor_utils get_supported_processors def get_supported_processors() -> dict Gets the list of supported processors. Returns dict - The list of supported processors. Examples >>> get_supported_processors() {<Processor.CORAL_EDGE_TPU: 1>: {'is_available': True, 'coral_tpus': [{'type': 'usb', 'path':\\ '/sys/bus/usb/devices/2-2'}]}, <Processor.CPU: 2>: {'is_available': True, 'cpu_count': 4}} get_perceptor_processor def get_perceptor_processor(processor_preference: list) -> Processor Gets the processor for the perceptor. Arguments processor_preference ( list ): The list of preferred processors. Returns Processor - The processor for the perceptor. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> get_perceptor_processor(processor_preference) Processor.CORAL_EDGE_TPU validate_processor_preference def validate_processor_preference(processor_preference, model_path_required=True) Validate the processor preference. Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, } >>> validate_processor_preference(processor_preference) validate_processor_list def validate_processor_list(processor_list) Validate the processor list. Arguments processor_list : A list of processors. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_list = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> validate_processor_list(processor_list)","title":"Perceptor Utils"},{"location":"perceptors/perceptor-utils/#darcyaiperceptorperceptor_utils","text":"","title":"darcyai.perceptor.perceptor_utils"},{"location":"perceptors/perceptor-utils/#get_supported_processors","text":"def get_supported_processors() -> dict Gets the list of supported processors. Returns dict - The list of supported processors. Examples >>> get_supported_processors() {<Processor.CORAL_EDGE_TPU: 1>: {'is_available': True, 'coral_tpus': [{'type': 'usb', 'path':\\ '/sys/bus/usb/devices/2-2'}]}, <Processor.CPU: 2>: {'is_available': True, 'cpu_count': 4}}","title":"get_supported_processors"},{"location":"perceptors/perceptor-utils/#get_perceptor_processor","text":"def get_perceptor_processor(processor_preference: list) -> Processor Gets the processor for the perceptor. Arguments processor_preference ( list ): The list of preferred processors. Returns Processor - The processor for the perceptor. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> get_perceptor_processor(processor_preference) Processor.CORAL_EDGE_TPU","title":"get_perceptor_processor"},{"location":"perceptors/perceptor-utils/#validate_processor_preference","text":"def validate_processor_preference(processor_preference, model_path_required=True) Validate the processor preference. Arguments processor_preference : A dictionary of processor preference. The key is the processor. Values are dictionaries of model paths and labels. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_preference = { Processor.CORAL_EDGE_TPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, Processor.CPU: { \"model_path\": \"/path/to/model.tflite\", \"labels_file\": \"/path/to/labels.txt\", }, } >>> validate_processor_preference(processor_preference)","title":"validate_processor_preference"},{"location":"perceptors/perceptor-utils/#validate_processor_list","text":"def validate_processor_list(processor_list) Validate the processor list. Arguments processor_list : A list of processors. Raises Exception : If the processor preference is invalid. Examples >>> from darcyai.perceptor.processor import Processor >>> processor_list = [Processor.CORAL_EDGE_TPU, Processor.CPU] >>> validate_processor_list(processor_list)","title":"validate_processor_list"},{"location":"perceptors/perceptor/","text":"darcyai.perceptor.perceptor Perceptor Objects class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass run def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor. load def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None . is_loaded def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise. set_loaded def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"Perceptor"},{"location":"perceptors/perceptor/#darcyaiperceptorperceptor","text":"","title":"darcyai.perceptor.perceptor"},{"location":"perceptors/perceptor/#perceptor-objects","text":"class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass","title":"Perceptor Objects"},{"location":"perceptors/perceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor.","title":"run"},{"location":"perceptors/perceptor/#load","text":"def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None .","title":"load"},{"location":"perceptors/perceptor/#is_loaded","text":"def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise.","title":"is_loaded"},{"location":"perceptors/perceptor/#set_loaded","text":"def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state.","title":"set_loaded"},{"location":"perceptors/perceptor/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"perceptors/perceptor/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"perceptors/perceptor/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"perceptors/coral-perceptors/coralperceptorbase/","text":"darcyai.perceptor.coral.coral_perceptor_base CoralPerceptorBase Objects class CoralPerceptorBase(Perceptor) Base class for all Coral Perceptors. load def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None . list_edge_tpus @staticmethod def list_edge_tpus() -> list Lists the Coral Edge TPUs. Returns list : The Coral Edge TPUs. Example >>> from darcyai.perceptor.coral.coral_perceptor_base import CoralPerceptorBase >>> CoralPerceptorBase.list_edge_tpus() [\"/device:coral:0\"]","title":"CoralPerceptorBase"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#darcyaiperceptorcoralcoral_perceptor_base","text":"","title":"darcyai.perceptor.coral.coral_perceptor_base"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#coralperceptorbase-objects","text":"class CoralPerceptorBase(Perceptor) Base class for all Coral Perceptors.","title":"CoralPerceptorBase Objects"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#load","text":"def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None .","title":"load"},{"location":"perceptors/coral-perceptors/coralperceptorbase/#list_edge_tpus","text":"@staticmethod def list_edge_tpus() -> list Lists the Coral Edge TPUs. Returns list : The Coral Edge TPUs. Example >>> from darcyai.perceptor.coral.coral_perceptor_base import CoralPerceptorBase >>> CoralPerceptorBase.list_edge_tpus() [\"/device:coral:0\"]","title":"list_edge_tpus"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.coral.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. mean ( float ): The mean of the image. std ( float ): The standard deviation of the image. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns list[Class] : A list of detected classes. load def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use.","title":"ImageClassificationPerceptor"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#darcyaiperceptorcoralimage_classification_perceptor","text":"","title":"darcyai.perceptor.coral.image_classification_perceptor"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. mean ( float ): The mean of the image. std ( float ): The standard deviation of the image.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns list[Class] : A list of detected classes.","title":"run"},{"location":"perceptors/coral-perceptors/imageclassificationperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use.","title":"load"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.coral.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns list[Object] : A list of detected objects. load def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use. Returns None","title":"ObjectDetectionPerceptor"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#darcyaiperceptorcoralobject_detection_perceptor","text":"","title":"darcyai.perceptor.coral.object_detection_perceptor"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns list[Object] : A list of detected objects.","title":"run"},{"location":"perceptors/coral-perceptors/objectdetectionperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments accelerator_idx ( int ): The index of the Edge TPU to use. Returns None","title":"load"},{"location":"perceptors/coral-perceptors/peopleperceptor/","text":"darcyai.perceptor.coral.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(CoralPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/coral-perceptors/peopleperceptor/#darcyaiperceptorcoralpeople_perceptor","text":"","title":"darcyai.perceptor.coral.people_perceptor"},{"location":"perceptors/coral-perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(CoralPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/","text":"darcyai.perceptor.cpu.cpu_perceptor_base CpuPerceptorBase Objects class CpuPerceptorBase(Perceptor) Base class for all CPU Perceptors. load def load() -> None Loads the perceptor. read_label_file @staticmethod def read_label_file(filename: str, has_ids: bool = True, encoding: str = \"UTF-8\") -> dict Reads the labels file. Arguments filename ( str ): The path to the labels file. has_ids ( bool ): Whether the labels file contains IDs. encoding ( str ): The encoding of the labels file. Returns dict : A dictionary containing the labels.","title":"CpuPerceptorBase"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#darcyaiperceptorcpucpu_perceptor_base","text":"","title":"darcyai.perceptor.cpu.cpu_perceptor_base"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#cpuperceptorbase-objects","text":"class CpuPerceptorBase(Perceptor) Base class for all CPU Perceptors.","title":"CpuPerceptorBase Objects"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#load","text":"def load() -> None Loads the perceptor.","title":"load"},{"location":"perceptors/cpu-perceptors/cpuperceptorbase/#read_label_file","text":"@staticmethod def read_label_file(filename: str, has_ids: bool = True, encoding: str = \"UTF-8\") -> dict Reads the labels file. Arguments filename ( str ): The path to the labels file. has_ids ( bool ): Whether the labels file contains IDs. encoding ( str ): The encoding of the labels file. Returns dict : A dictionary containing the labels.","title":"read_label_file"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.cpu.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(CpuPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns (list[Any], list(str)) : A tuple containing the detected classes and the labels. load def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): Not used.","title":"ImageClassificationPerceptor"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#darcyaiperceptorcpuimage_classification_perceptor","text":"","title":"darcyai.perceptor.cpu.image_classification_perceptor"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(CpuPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments threshold ( float ): The threshold for object detection. top_k ( int ): The number of top predictions to return. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1.","title":"ImageClassificationPerceptor Objects"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Class] Runs the image classification model. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the perceptor. Returns (list[Any], list(str)) : A tuple containing the detected classes and the labels.","title":"run"},{"location":"perceptors/cpu-perceptors/imageclassificationperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments accelerator_idx ( int ): Not used.","title":"load"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.cpu.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(CpuPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1. run def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns (list[Any], list(str)) : A tuple containing the detected objects and the labels. load def load(accelerator_idx: [int, None] = None) -> None Loads the object detection model. Arguments accelerator_idx ( int ): Not used.","title":"ObjectDetectionPerceptor"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#darcyaiperceptorcpuobject_detection_perceptor","text":"","title":"darcyai.perceptor.cpu.object_detection_perceptor"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(CpuPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments threshold ( float ): The threshold for object detection. labels_file ( str ): The path to the labels file. labels ( dict ): A dictionary of labels. quantized ( bool ): Whether the model is quantized. num_cpu_threads ( int ): The number of threads to use for inference (CPU). Defaults to 1.","title":"ObjectDetectionPerceptor Objects"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> List[Object] Runs the object detection model on the input data. Arguments input_data ( Any ): The input data to run the model on. config ( ConfigRegistry ): The configuration for the Perceptor. Returns (list[Any], list(str)) : A tuple containing the detected objects and the labels.","title":"run"},{"location":"perceptors/cpu-perceptors/objectdetectionperceptor/#load","text":"def load(accelerator_idx: [int, None] = None) -> None Loads the object detection model. Arguments accelerator_idx ( int ): Not used.","title":"load"},{"location":"perceptors/cpu-perceptors/peopleperceptor/","text":"darcyai.perceptor.cpu.people_perceptor PeoplePerceptor Objects class PeoplePerceptor(CpuPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor"},{"location":"perceptors/cpu-perceptors/peopleperceptor/#darcyaiperceptorcpupeople_perceptor","text":"","title":"darcyai.perceptor.cpu.people_perceptor"},{"location":"perceptors/cpu-perceptors/peopleperceptor/#peopleperceptor-objects","text":"class PeoplePerceptor(CpuPerceptorBase, PeoplePerceptorBase) Perceptor for detecting people in an image. Perceptor Config: minimum_face_threshold (float): Confidence threshold for detecting the face as a percent certainty Default value: 0.4 minimum_body_threshold (float): Confidence threshold for detecting the body as a percent certainty Default value: 0.2 minimum_face_height (int): The minimum height of the persons face in pixels that we want to work with Default value: 20 minimum_body_height (int): The minimum height of the persons body in pixels that we want to work with Default value: 120 show_pose_landmark_dots (bool): Show pose landmark dots (nose, ears, elbows, etc.) Default value: False show_body_rectangle (bool): Draw a rectangle around the persons body Default value: False show_face_rectangle (bool): Draw a rectangle around the persons face Default value: False face_rectangle_color (rgb): Color of the face rectangle Default value: RGB(255, 0, 0) face_rectangle_thickness (int): Thickness of the face rectangle Default value: 1 body_rectangle_color (rgb): Color of the body rectangle Default value: RGB(0, 255, 0) body_rectangle_thickness (int): Thickness of the body rectangle Default value: 1 pose_landmark_dot_confidence_threshold (float): Confidence threshold for identifying pose landmarks as a percent certainty Default value: 0.5 pose_landmark_dot_size (int): Size of the pose landmark dots Default value: 1 pose_landmark_dot_color (rgb): Color of the pose landmark dots Default value: RGB(255, 255, 255) show_face_position_arrow (bool): Show the arrow that indicates which direction the person is looking Default value: False face_position_arrow_color (rgb): Color of the face position arrow Default value: RGB(255, 255, 255) face_position_arrow_stroke (int): Thickness of the face position arrow Default value: 1 face_position_arrow_offset_x (int): X offset for the face position arrow Default value: 0 face_position_arrow_offset_y (int): Y offset for the face position arrow Default value: -30 face_position_arrow_length (int): Length of the face position arrow Default value: 20 face_position_left_right_threshold (float): Threshold for detecting if the person is looking left or right Default value: 0.3 face_position_straight_threshold (float): Threshold for detecting if the person is looking straight Default value: 0.7 show_forehead_center_dot (bool): Show a dot in the center of the persons forehead Default value: False forehead_center_dot_color (rgb): Color of the forehead center dot Default value: RGB(255, 255, 255) forehead_center_dot_size (int): Size of the forehead center dot Default value: 1 face_rectangle_y_factor (float): Size adjustment factor for the height of the persons face, which can be used to make sure objects like hair and hats are captured Default value: 1.0 show_centroid_dots (bool): Show centroid information (center of the face or body) Default value: False centroid_dots_color (rgb): Color of the centroid information Default value: RGB(255, 255, 255) centroid_dots_size (int): Size of the centroid information Default value: 1 object_tracking_allowed_missed_frames (int): Object tracking allowed missed frames Default value: 50 object_tracking_color_sample_pixels (int): Number of pixels to use for color sampling when tracking objects Default value: 4 object_tracking_info_history_count (int): Number of video frames used to track an object in the field of view Default value: 3 object_tracking_removal_count (int): Number of frames to wait before removing an object from tracking Default value: 50 object_tracking_centroid_weight (float): Level of importance that centroid data has when tracking objects Default value: 0.25 object_tracking_color_weight (float): Level of importance that color data has when tracking objects Default value: 0.25 object_tracking_vector_weight (float): Level of importance that vector data has when tracking objects Default value: 0.25 object_tracking_size_weight (float): Level of importance that size data has when tracking objects Default value: 0.25 object_tracking_creation_m (int): Minimum number of frames out of N frames that an object must be present in the field of view before it is tracked Default value: 10 object_tracking_creation_n (int): Total number of frames used to evaluate an object before it is tracked Default value: 7 person_tracking_creation_m (int): Minimum number of frames out of N frames needed to promote a tracked object to a person Default value: 20 person_tracking_creation_m (int): Total number of frames used to evaluate a tracked object before it is promoted to a person Default value: 16 show_person_id (bool): Show anonymous unique identifier for the person Default value: False person_data_line_color (rgb): Color of the person data line Default value: RGB(255, 255, 255) person_data_line_thickness (int): Thickness of the person data line Default value: 1 person_data_identity_text_color (rgb): Color of the person data identity text Default value: RGB(255, 255, 255) person_data_identity_text_stroke (int): Thickness of the person data identity text Default value: 1 person_data_identity_text_font_size (float): Font size of the person data identity text Default value: 1.0 person_data_text_offset_x (int): X offset of the person data text Default value: 30 person_data_text_offset_y (int): Y offset of the person data text Default value: -40 identity_text_prefix (str): Text information that you want to display at the beginning of the person ID Default value: Person ID: rolling_video_storage_frame_count (int): Number of the video frames to store while processing Default value: 100 Events: new_person_entered_scene person_facing_new_direction new_person_in_front person_left_scene identity_determined_for_person person_got_too_close person_went_too_far_away max_person_count_reached person_count_fell_below_maximum person_occluded","title":"PeoplePerceptor Objects"},{"location":"pipeline-config/config/","text":"darcyai.config Config Objects class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. label ( str ): The label of the config. config_type ( str ): The type of the config. Valid types are: int float bool str rgb default_value ( Any ): The default value of the config. description ( str ): The description of the config. is_valid def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise. cast def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"Config"},{"location":"pipeline-config/config/#darcyaiconfig","text":"","title":"darcyai.config"},{"location":"pipeline-config/config/#config-objects","text":"class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. label ( str ): The label of the config. config_type ( str ): The type of the config. Valid types are: int float bool str rgb default_value ( Any ): The default value of the config. description ( str ): The description of the config.","title":"Config Objects"},{"location":"pipeline-config/config/#is_valid","text":"def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise.","title":"is_valid"},{"location":"pipeline-config/config/#cast","text":"def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"cast"},{"location":"pipeline-config/rgb/","text":"darcyai.config RGB Objects class RGB() Class to hold the configuration for the RGB. Arguments red ( int ): The red value. green ( int ): The green value. blue ( int ): The blue value. red def red() -> int Returns the red value. Returns int : The red value. green def green() -> int Returns the green value. Returns int : The green value. blue def blue() -> int Returns the blue value. Returns int : The blue value. to_hex def to_hex() -> str Returns the hex value of the RGB. Returns str : The hex value. from_string @staticmethod def from_string(rgb: str) -> \"RGB\" Creates an RGB object from a comma separated RGB string. Arguments rgb ( str ): The comma separated RGB string. Returns RGB : The RGB object. Examples >>> RGB.from_string(\"255,255,255\") from_hex_string @staticmethod def from_hex_string(hex_rgb: str) -> \"RGB\" Creates an RGB object from a hex RGB string. Arguments hex_rgb ( str ): The hex RGB string. Returns RGB : The RGB object.","title":"RGB"},{"location":"pipeline-config/rgb/#darcyaiconfig","text":"","title":"darcyai.config"},{"location":"pipeline-config/rgb/#rgb-objects","text":"class RGB() Class to hold the configuration for the RGB. Arguments red ( int ): The red value. green ( int ): The green value. blue ( int ): The blue value.","title":"RGB Objects"},{"location":"pipeline-config/rgb/#red","text":"def red() -> int Returns the red value. Returns int : The red value.","title":"red"},{"location":"pipeline-config/rgb/#green","text":"def green() -> int Returns the green value. Returns int : The green value.","title":"green"},{"location":"pipeline-config/rgb/#blue","text":"def blue() -> int Returns the blue value. Returns int : The blue value.","title":"blue"},{"location":"pipeline-config/rgb/#to_hex","text":"def to_hex() -> str Returns the hex value of the RGB. Returns str : The hex value.","title":"to_hex"},{"location":"pipeline-config/rgb/#from_string","text":"@staticmethod def from_string(rgb: str) -> \"RGB\" Creates an RGB object from a comma separated RGB string. Arguments rgb ( str ): The comma separated RGB string. Returns RGB : The RGB object. Examples >>> RGB.from_string(\"255,255,255\")","title":"from_string"},{"location":"pipeline-config/rgb/#from_hex_string","text":"@staticmethod def from_hex_string(hex_rgb: str) -> \"RGB\" Creates an RGB object from a hex RGB string. Arguments hex_rgb ( str ): The hex RGB string. Returns RGB : The RGB object.","title":"from_hex_string"}]}